{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOguOjBEIh1/ihhGMq2+CnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imZiho/Deep-Learning_Seminar/blob/main/CHA06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y fonts-nanum* | tail -n 1\n",
        "!sudo fc-cache -fv\n",
        "!rm -rf ~/.cache/matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwJ83wZ54YXH",
        "outputId": "ddb38bb1-a2ba-4209-9863-37c3872dc2cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 39 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요 라이브러리 설치\n",
        "\n",
        "!pip install torchviz | tail -n 1\n",
        "!pip install torchinfo | tail -n 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmXitp5G4Z_1",
        "outputId": "8731b6fb-7ec7-4d8c-ddfd-f7c8d7c232a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed torchviz-0.0.2\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 임포트\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "# 폰트 관련 용도\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 나눔 고딕 폰트의 경로 명시\n",
        "path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "font_name = fm.FontProperties(fname=path, size=10).get_name()"
      ],
      "metadata": {
        "id": "jmPF2lSH4c5u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이토치 관련 라이브러리\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "from torchviz import make_dot"
      ],
      "metadata": {
        "id": "V05EGtpu4fLD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 폰트 설정\n",
        "plt.rcParams['font.family'] = font_name\n",
        "\n",
        "# 기본 폰트 사이즈 변경\n",
        "plt.rcParams['font.size'] = 14\n",
        "\n",
        "# 기본 그래프 사이즈 변경\n",
        "plt.rcParams['figure.figsize'] = (6,6)\n",
        "\n",
        "# 기본 그리드 표시\n",
        "# 필요에 따라 설정할 때는, plt.grid()\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "# 마이너스 기호 정상 출력\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 넘파이 부동소수점 자릿수 표시\n",
        "np.set_printoptions(suppress=True, precision=4)"
      ],
      "metadata": {
        "id": "vITld8o64g39"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **이진 로지스틱 회귀 모델**\n",
        "\n",
        "분류 모델은 데이터를 분류하는 방법으로 크게 이진 분류(Binary Classification)와 다중 분류(Multi Classification)로 나뉜다.\n",
        "두 개의 선택지 중에서 하나를 결정하는 문제를 **이진 분류(Binary Classification)**라고 한다.\n",
        "\n",
        "\n",
        "그리고 이런 문제를 풀기 위한 대표적인 알고리즘은 **로지스틱 회귀(Logistic Regression)**라고 한다.\n",
        "\n",
        "\n",
        "**앞장의 선형회귀 모델과 비교하면 예측 모델에 시그모이드 함수의 개념이 추가로 들어간다**\n",
        "\n",
        "정확도:(정답건수)/(전체건수)\n",
        "\n",
        "\n",
        "모델이 어느정도 비율로 올바른 예측이 가능한지 수치화할 수 있음"
      ],
      "metadata": {
        "id": "8j_8Yw-vvA9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 시그모이드 함수\n",
        "\n",
        "**예측결과가 1,예측결과가0이라고 하였을 때 그래프를 그려보면 S자로 시그모이드함수 형태임**\n",
        "\n",
        "따라서,출력이 0과 1사이의 값을 가지면서 S자 형태로 그려지는 함수= 시그모이드 함수(Sigmoid function)"
      ],
      "metadata": {
        "id": "W4Ml_3zH1DZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "O7nO2ICh3mVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_np = np.arange(-4,4.1,0.25)           # 넘파이를 이용하여 x_np를 정의한 후\n",
        "x = torch.tensor(x_np).float()          # 텐서로 변환하여 변수 x에 저장하고\n",
        "y = torch.sigmoid(x)                    # 그 값을 시그모이드 함수로 y값을 계산해줌 (파이토치에서는 torch.sigmoid를 이용하여 시그모이드 구현가능)\n",
        "\n",
        "plt.title('시그모이드 함수의 그래프')\n",
        "plt.plot(x.data,y.data)\n",
        "plt.show()                              # 그래프는 (0.0.5)를 기준으로 점대칭`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "exm5IFLKxJp2",
        "outputId": "e39b4ed2-810d-4ebf-cdb6-9cbc1ac11172"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49884 (\\N{HANGUL SYLLABLE SI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44536 (\\N{HANGUL SYLLABLE GEU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 47784 (\\N{HANGUL SYLLABLE MO}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 46300 (\\N{HANGUL SYLLABLE DEU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54632 (\\N{HANGUL SYLLABLE HAM}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51032 (\\N{HANGUL SYLLABLE YI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 47000 (\\N{HANGUL SYLLABLE RAE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54532 (\\N{HANGUL SYLLABLE PEU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**실습**\n",
        "\n",
        "\n",
        "**데이터준비**"
      ],
      "metadata": {
        "id": "_NWBk61b8nve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris    # 라이브러리를 임포트 해온 후\n",
        "iris = load_iris()                        # 데이터를 불러오고\n",
        "x_org,y_org=iris.data,iris.target\n",
        "print('원본데이터',x_org.shape,y_org.shape)  #결과확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-rFnP5a8qn-",
        "outputId": "3b70551b-6033-4def-86c3-b02ac2356e3b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본데이터 (150, 4) (150,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**우리가 사용할 데이터셋은 머신러닝과 통계 분야에서 오래전부터 사용해온 붓꽃iris 데이터셋이다**\n",
        "\n",
        "**이 데이터는 scikit-learn의 datasets 모듈에 포함되어 있가에 라이브러리를 임포트 해온 후**\n",
        "\n",
        "**load_iris 함수를 사용해서 데이터를 불러와 결과를 확인해보면\n",
        "데이터셋에는 총 150개의 데이터가 담겨있고, 각 데이터에는 4개의 정보(sepal,petal의 길이(length)와 폭(width))가 담겨있는것을 확인할 수 있습니다)**"
      ],
      "metadata": {
        "id": "a1e2VCqULsey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 추출\n",
        "x_data = iris.data[:100,:2]   # [행:열: 인덱스 범위]  # 이제 데이터르 추출해줄건데 100행,2열까지만 추출함\n",
        "y_data = iris.target[:100]\n",
        "print('대상 데이터',x_data.shape,y_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB8I5k_YM3tI",
        "outputId": "cea25e41-fc43-40b7-fe7e-df280606f4c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "대상 데이터 (100, 2) (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련데이터와 검증 데이터의 분할\n",
        "print(x_data.shape,y_data.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_data, y_data, train_size=70, test_size=30,\n",
        "    random_state=123)\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMQ9dS7bOZ2f",
        "outputId": "5ba00b5a-a9da-4e88-a057-cccac0a1b29f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 2) (100,)\n",
            "(70, 2) (30, 2) (70,) (30,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**우리는 이 모델의 예측을 신뢰할 수 있는지 알아야 한다.\n",
        "따라서 모델의 성능을 측정하려면 이전에 본 적 없는 새 데이터를 모델에 적용해봐야 한다.**\n",
        "\n",
        "**이를 위해 우리가 가지고 있는 레이블된 데이터(150개의 붓꽃 데이터)를 두 그룹으로 하나는 머신러닝 모델을 만들 때 사용하는 훈련 데이터, 나머지는 모델이 얼마나 잘 작동하는지 측정하는 데 사용하는 테스트 데이터로 나눔**\n",
        "\n",
        "\n",
        "**scikit-learn은 데이터셋을 섞어서 나눠주는 train_test_split 함수를 제공한다**\n",
        "\n",
        "\n",
        "이 함수는 전체 행 중 75% 를 레이블 데이터와 함께 훈련 세트로 뽑고,\n",
        "나머지 25%는 레이블 데이터와 함께 테스트 세트가 된다.\n",
        "\n",
        "나누는 비율에 관한 정해진 규칙은 없지만 대부분 훈련:검증을 대략 7:3이나 6:4로 나눈것이 일반적.\n",
        "\n",
        "\n",
        "**따라서 train_test_split 모듈을 임포트 해온 후 train_test_split 함수의 반환값은 X_train, X_test, y_train, y_test이며 모두 NumPy 배열입니다. **\n",
        "\n",
        "\n",
        "\n",
        "**이 위의 코드로 데이터를 훈련용과 검증데이터로 나눈 후 결과를 확인해보면 70개의 학습데이터 30개의 테스트데이터로 나누어진것을 확인할 수 있고 이러한 분리된 검증데이터는 모델의 정확도를 평가하는 목적으로만 사용된다**"
      ],
      "metadata": {
        "id": "0RNQy9ZhPBWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 산포도출력\n",
        "x_t0 = x_train[y_train == 0]\n",
        "x_t1 = x_train[y_train == 1]\n",
        "plt.scatter(x_t0[:,0], x_t0[:,1], marker='x', c='b', label='0 (setosa)')\n",
        "plt.scatter(x_t1[:,0], x_t1[:,1], marker='o', c='k', label='1 (versicolor)')\n",
        "plt.xlabel('sepal_length')\n",
        "plt.ylabel('sepal_width')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZtfHqYndST__",
        "outputId": "e4f0124f-3959-4e69-be82-35e529db52b3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'NanumGothic' not found.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "산포도 출력결과 두 그룹으로 분할이 가능해보이는것을 확인할 수 있다"
      ],
      "metadata": {
        "id": "vdgZEkqTSpb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**모델정의**"
      ],
      "metadata": {
        "id": "HPEdmuTBg9Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#입출력 차원수의 정의\n",
        "n_input = x_train.shape[1]\n",
        "n_output = 1\n",
        "print(f'n_input:{n_input}n_output:{n_output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJZbn8ddUQRO",
        "outputId": "21344ea6-a60a-4819-c4ea-c7a3f5f5f1f0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_input:2n_output:1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2입력1출력의 로지스틱 회귀 모델(모델구현)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_input, n_output):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(n_input, n_output)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # 초깃값을 전부 1로 함\n",
        "        # \"딥러닝을 위한 수학\"과 조건을 맞추기 위한 목적\n",
        "        self.l1.weight.data.fill_(1.0)\n",
        "        self.l1.bias.data.fill_(1.0)\n",
        "\n",
        "    # 예측 함수 정의\n",
        "    def forward(self, x):\n",
        "        # 선형 함수에 입력값을 넣고 계산한 결과\n",
        "        x1 = self.l1(x)\n",
        "        # 계산 결과에 시그모이드 함수를 적용\n",
        "        x2 = self.sigmoid(x1)\n",
        "        return x2"
      ],
      "metadata": {
        "id": "b7tvZo7CY-VC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(n_input, n_output)"
      ],
      "metadata": {
        "id": "LFfiSZccbbud"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(net)  #모델 개요 표시방법"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFytr92JbQcu",
        "outputId": "e7698546-f5b5-48ed-d42b-b52ccdd8d8d8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (l1): Linear(in_features=2, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(net, (2,))   #모델 개요 표시방법"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWGbvCr_bYHN",
        "outputId": "6c2d6d2a-dffb-4769-e81f-4c564ae3bbf3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Net                                      [1]                       --\n",
              "├─Linear: 1-1                            [1]                       3\n",
              "├─Sigmoid: 1-2                           [1]                       --\n",
              "==========================================================================================\n",
              "Total params: 3\n",
              "Trainable params: 3\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#최적화 알고리즘과 손실 함수의 정의\n",
        "criterion = nn.BCELoss()      # 분류모델에서 손실함수는 교차 엔트로피 함수를 이용한다\n",
        "\n",
        "# 학습률\n",
        "lr = 0.01\n",
        "\n",
        "# 최적화 함수: 경사 하강법\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "AYYY4TaGbZqt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터 x_train과 정답 데이터 y_train의 텐서화\n",
        "\n",
        "inputs = torch.tensor(x_train).float()\n",
        "labels = torch.tensor(y_train).float()\n",
        "\n",
        "# 정답 데이터는 N행 1열 행렬로 변환\n",
        "labels1 = labels.view((-1,1))\n",
        "\n",
        "# 검증 데이터의 텐서화\n",
        "inputs_test = torch.tensor(x_test).float()\n",
        "labels_test = torch.tensor(y_test).float()\n",
        "\n",
        "# 검증용 정답 데이터도 N행 1열 행렬로 변환\n",
        "labels1_test = labels_test.view((-1,1))\n",
        "\n",
        "# 예측 계산\n",
        "outputs = net(inputs)\n",
        "\n",
        "# 손실 계산\n",
        "loss = criterion(outputs, labels1)\n",
        "\n",
        "# 손실을 계산 그래프로 출력\n",
        "g = make_dot(loss, params=dict(net.named_parameters()))\n",
        "display(g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "94NfTOa1biyY",
        "outputId": "3299ff06-72d6-49cb-d00e-c7b4b9ab50e6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"216pt\" height=\"391pt\"\n viewBox=\"0.00 0.00 216.00 391.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 387)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-387 212,-387 212,4 -4,4\"/>\n<!-- 138649640533136 -->\n<g id=\"node1\" class=\"node\">\n<title>138649640533136</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"130.5,-31 76.5,-31 76.5,0 130.5,0 130.5,-31\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 138649643622288 -->\n<g id=\"node2\" class=\"node\">\n<title>138649643622288</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"193,-86 14,-86 14,-67 193,-67 193,-86\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">BinaryCrossEntropyBackward0</text>\n</g>\n<!-- 138649643622288&#45;&gt;138649640533136 -->\n<g id=\"edge8\" class=\"edge\">\n<title>138649643622288&#45;&gt;138649640533136</title>\n<path fill=\"none\" stroke=\"black\" d=\"M103.5,-66.79C103.5,-60.07 103.5,-50.4 103.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107,-41.19 103.5,-31.19 100,-41.19 107,-41.19\"/>\n</g>\n<!-- 138649643620224 -->\n<g id=\"node3\" class=\"node\">\n<title>138649643620224</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"160,-141 47,-141 47,-122 160,-122 160,-141\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">SigmoidBackward0</text>\n</g>\n<!-- 138649643620224&#45;&gt;138649643622288 -->\n<g id=\"edge1\" class=\"edge\">\n<title>138649643620224&#45;&gt;138649643622288</title>\n<path fill=\"none\" stroke=\"black\" d=\"M103.5,-121.75C103.5,-114.8 103.5,-104.85 103.5,-96.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107,-96.09 103.5,-86.09 100,-96.09 107,-96.09\"/>\n</g>\n<!-- 138649643629152 -->\n<g id=\"node4\" class=\"node\">\n<title>138649643629152</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"154,-196 53,-196 53,-177 154,-177 154,-196\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 138649643629152&#45;&gt;138649643620224 -->\n<g id=\"edge2\" class=\"edge\">\n<title>138649643629152&#45;&gt;138649643620224</title>\n<path fill=\"none\" stroke=\"black\" d=\"M103.5,-176.75C103.5,-169.8 103.5,-159.85 103.5,-151.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107,-151.09 103.5,-141.09 100,-151.09 107,-151.09\"/>\n</g>\n<!-- 138649641227632 -->\n<g id=\"node5\" class=\"node\">\n<title>138649641227632</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-251 0,-251 0,-232 101,-232 101,-251\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 138649641227632&#45;&gt;138649643629152 -->\n<g id=\"edge3\" class=\"edge\">\n<title>138649641227632&#45;&gt;138649643629152</title>\n<path fill=\"none\" stroke=\"black\" d=\"M59.25,-231.75C66.97,-224.03 78.4,-212.6 87.72,-203.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"90.31,-205.64 94.91,-196.09 85.36,-200.69 90.31,-205.64\"/>\n</g>\n<!-- 138649639546736 -->\n<g id=\"node6\" class=\"node\">\n<title>138649639546736</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"80,-317 21,-317 21,-287 80,-287 80,-317\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\">l1.bias</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 138649639546736&#45;&gt;138649641227632 -->\n<g id=\"edge4\" class=\"edge\">\n<title>138649639546736&#45;&gt;138649641227632</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-286.84C50.5,-279.21 50.5,-269.7 50.5,-261.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-261.27 50.5,-251.27 47,-261.27 54,-261.27\"/>\n</g>\n<!-- 138649641233008 -->\n<g id=\"node7\" class=\"node\">\n<title>138649641233008</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"196,-251 119,-251 119,-232 196,-232 196,-251\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 138649641233008&#45;&gt;138649643629152 -->\n<g id=\"edge5\" class=\"edge\">\n<title>138649641233008&#45;&gt;138649643629152</title>\n<path fill=\"none\" stroke=\"black\" d=\"M148.58,-231.75C140.72,-224.03 129.07,-212.6 119.58,-203.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"121.84,-200.6 112.25,-196.09 116.94,-205.59 121.84,-200.6\"/>\n</g>\n<!-- 138649641224176 -->\n<g id=\"node8\" class=\"node\">\n<title>138649641224176</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-311.5 107,-311.5 107,-292.5 208,-292.5 208,-311.5\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-299.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 138649641224176&#45;&gt;138649641233008 -->\n<g id=\"edge6\" class=\"edge\">\n<title>138649641224176&#45;&gt;138649641233008</title>\n<path fill=\"none\" stroke=\"black\" d=\"M157.5,-292.37C157.5,-284.25 157.5,-271.81 157.5,-261.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"161,-261.17 157.5,-251.17 154,-261.17 161,-261.17\"/>\n</g>\n<!-- 138649975086976 -->\n<g id=\"node9\" class=\"node\">\n<title>138649975086976</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"193,-383 122,-383 122,-353 193,-353 193,-383\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\">l1.weight</text>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-360\" font-family=\"monospace\" font-size=\"10.00\"> (1, 2)</text>\n</g>\n<!-- 138649975086976&#45;&gt;138649641224176 -->\n<g id=\"edge7\" class=\"edge\">\n<title>138649975086976&#45;&gt;138649641224176</title>\n<path fill=\"none\" stroke=\"black\" d=\"M157.5,-352.8C157.5,-343.7 157.5,-331.79 157.5,-321.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"161,-321.84 157.5,-311.84 154,-321.84 161,-321.84\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7e19e2c1a410>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습률\n",
        "lr = 0.01\n",
        "\n",
        "# 초기화\n",
        "net = Net(n_input, n_output)\n",
        "\n",
        "# 손실 함수： 교차 엔트로피 함수\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# 최적화 함수: 경사 하강법\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "# 반복 횟수\n",
        "num_epochs = 10000\n",
        "\n",
        "# 기록용 리스트 초기화\n",
        "history = np.zeros((0,5))"
      ],
      "metadata": {
        "id": "R_kIeQJObo9r"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 반복 계산 메인 루프\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # 훈련 페이즈\n",
        "\n",
        "    # 경삿값 초기화\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 예측 계산\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # 손실 계산\n",
        "    loss = criterion(outputs, labels1)\n",
        "\n",
        "    # 경사 계산\n",
        "    loss.backward()\n",
        "\n",
        "    # 파라미터 수정\n",
        "    optimizer.step()\n",
        "\n",
        "    # 손실 저장(스칼라 값 취득)\n",
        "    train_loss = loss.item()\n",
        "\n",
        "    # 예측 라벨(1 또는 0) 계산\n",
        "    predicted = torch.where(outputs < 0.5, 0, 1)\n",
        "\n",
        "    # 정확도 계산\n",
        "    train_acc = (predicted == labels1).sum() / len(y_train)\n",
        "\n",
        "    # 예측 페이즈\n",
        "\n",
        "    # 예측 계산\n",
        "    outputs_test = net(inputs_test)\n",
        "\n",
        "    # 손실 계산\n",
        "    loss_test = criterion(outputs_test, labels1_test)\n",
        "\n",
        "    # 손실 저장(스칼라 값 취득)\n",
        "    val_loss =  loss_test.item()\n",
        "\n",
        "    # 예측 라벨(1 또는 0) 계산\n",
        "    predicted_test = torch.where(outputs_test < 0.5, 0, 1)\n",
        "\n",
        "    # 정확도 계산\n",
        "    val_acc = (predicted_test == labels1_test).sum() / len(y_test)\n",
        "\n",
        "    if ( epoch % 10 == 0):\n",
        "        print (f'Epoch [{epoch}/{num_epochs}], loss: {train_loss:.5f} acc: {train_acc:.5f} val_loss: {val_loss:.5f}, val_acc: {val_acc:.5f}')\n",
        "        item = np.array([epoch, train_loss, train_acc, val_loss, val_acc])\n",
        "        history = np.vstack((history, item))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_RucHgibsAo",
        "outputId": "9c3b5dd1-db49-4de0-eb04-13699a7c1273"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/10000], loss: 4.77289 acc: 0.50000 val_loss: 4.49384, val_acc: 0.50000\n",
            "Epoch [10/10000], loss: 3.80546 acc: 0.50000 val_loss: 3.56537, val_acc: 0.50000\n",
            "Epoch [20/10000], loss: 2.84329 acc: 0.50000 val_loss: 2.64328, val_acc: 0.50000\n",
            "Epoch [30/10000], loss: 1.91613 acc: 0.50000 val_loss: 1.76244, val_acc: 0.50000\n",
            "Epoch [40/10000], loss: 1.17137 acc: 0.50000 val_loss: 1.08537, val_acc: 0.50000\n",
            "Epoch [50/10000], loss: 0.84140 acc: 0.50000 val_loss: 0.81872, val_acc: 0.50000\n",
            "Epoch [60/10000], loss: 0.77087 acc: 0.50000 val_loss: 0.77093, val_acc: 0.50000\n",
            "Epoch [70/10000], loss: 0.75450 acc: 0.34286 val_loss: 0.76105, val_acc: 0.33333\n",
            "Epoch [80/10000], loss: 0.74542 acc: 0.25714 val_loss: 0.75447, val_acc: 0.20000\n",
            "Epoch [90/10000], loss: 0.73734 acc: 0.24286 val_loss: 0.74778, val_acc: 0.16667\n",
            "Epoch [100/10000], loss: 0.72949 acc: 0.24286 val_loss: 0.74098, val_acc: 0.13333\n",
            "Epoch [110/10000], loss: 0.72180 acc: 0.27143 val_loss: 0.73419, val_acc: 0.16667\n",
            "Epoch [120/10000], loss: 0.71423 acc: 0.31429 val_loss: 0.72749, val_acc: 0.20000\n",
            "Epoch [130/10000], loss: 0.70680 acc: 0.41429 val_loss: 0.72087, val_acc: 0.20000\n",
            "Epoch [140/10000], loss: 0.69949 acc: 0.47143 val_loss: 0.71437, val_acc: 0.26667\n",
            "Epoch [150/10000], loss: 0.69230 acc: 0.52857 val_loss: 0.70797, val_acc: 0.30000\n",
            "Epoch [160/10000], loss: 0.68524 acc: 0.60000 val_loss: 0.70167, val_acc: 0.36667\n",
            "Epoch [170/10000], loss: 0.67829 acc: 0.62857 val_loss: 0.69548, val_acc: 0.43333\n",
            "Epoch [180/10000], loss: 0.67147 acc: 0.68571 val_loss: 0.68938, val_acc: 0.50000\n",
            "Epoch [190/10000], loss: 0.66476 acc: 0.75714 val_loss: 0.68339, val_acc: 0.56667\n",
            "Epoch [200/10000], loss: 0.65816 acc: 0.81429 val_loss: 0.67749, val_acc: 0.70000\n",
            "Epoch [210/10000], loss: 0.65168 acc: 0.84286 val_loss: 0.67169, val_acc: 0.70000\n",
            "Epoch [220/10000], loss: 0.64531 acc: 0.85714 val_loss: 0.66599, val_acc: 0.73333\n",
            "Epoch [230/10000], loss: 0.63904 acc: 0.85714 val_loss: 0.66037, val_acc: 0.76667\n",
            "Epoch [240/10000], loss: 0.63288 acc: 0.88571 val_loss: 0.65485, val_acc: 0.80000\n",
            "Epoch [250/10000], loss: 0.62682 acc: 0.88571 val_loss: 0.64942, val_acc: 0.83333\n",
            "Epoch [260/10000], loss: 0.62087 acc: 0.90000 val_loss: 0.64408, val_acc: 0.83333\n",
            "Epoch [270/10000], loss: 0.61501 acc: 0.91429 val_loss: 0.63882, val_acc: 0.83333\n",
            "Epoch [280/10000], loss: 0.60925 acc: 0.92857 val_loss: 0.63364, val_acc: 0.86667\n",
            "Epoch [290/10000], loss: 0.60359 acc: 0.94286 val_loss: 0.62855, val_acc: 0.90000\n",
            "Epoch [300/10000], loss: 0.59803 acc: 0.94286 val_loss: 0.62354, val_acc: 0.90000\n",
            "Epoch [310/10000], loss: 0.59255 acc: 0.94286 val_loss: 0.61861, val_acc: 0.90000\n",
            "Epoch [320/10000], loss: 0.58717 acc: 0.94286 val_loss: 0.61376, val_acc: 0.93333\n",
            "Epoch [330/10000], loss: 0.58187 acc: 0.94286 val_loss: 0.60899, val_acc: 0.93333\n",
            "Epoch [340/10000], loss: 0.57667 acc: 0.97143 val_loss: 0.60429, val_acc: 0.93333\n",
            "Epoch [350/10000], loss: 0.57154 acc: 0.97143 val_loss: 0.59967, val_acc: 0.93333\n",
            "Epoch [360/10000], loss: 0.56650 acc: 0.97143 val_loss: 0.59512, val_acc: 0.93333\n",
            "Epoch [370/10000], loss: 0.56155 acc: 0.98571 val_loss: 0.59064, val_acc: 0.93333\n",
            "Epoch [380/10000], loss: 0.55667 acc: 0.98571 val_loss: 0.58623, val_acc: 0.93333\n",
            "Epoch [390/10000], loss: 0.55188 acc: 0.98571 val_loss: 0.58189, val_acc: 0.93333\n",
            "Epoch [400/10000], loss: 0.54716 acc: 0.98571 val_loss: 0.57762, val_acc: 0.93333\n",
            "Epoch [410/10000], loss: 0.54251 acc: 0.98571 val_loss: 0.57341, val_acc: 0.93333\n",
            "Epoch [420/10000], loss: 0.53795 acc: 0.98571 val_loss: 0.56927, val_acc: 0.93333\n",
            "Epoch [430/10000], loss: 0.53345 acc: 1.00000 val_loss: 0.56519, val_acc: 0.93333\n",
            "Epoch [440/10000], loss: 0.52902 acc: 1.00000 val_loss: 0.56117, val_acc: 0.93333\n",
            "Epoch [450/10000], loss: 0.52467 acc: 1.00000 val_loss: 0.55722, val_acc: 0.93333\n",
            "Epoch [460/10000], loss: 0.52038 acc: 1.00000 val_loss: 0.55333, val_acc: 0.93333\n",
            "Epoch [470/10000], loss: 0.51617 acc: 1.00000 val_loss: 0.54949, val_acc: 0.93333\n",
            "Epoch [480/10000], loss: 0.51201 acc: 1.00000 val_loss: 0.54571, val_acc: 0.93333\n",
            "Epoch [490/10000], loss: 0.50793 acc: 1.00000 val_loss: 0.54199, val_acc: 0.93333\n",
            "Epoch [500/10000], loss: 0.50390 acc: 1.00000 val_loss: 0.53833, val_acc: 0.93333\n",
            "Epoch [510/10000], loss: 0.49994 acc: 1.00000 val_loss: 0.53472, val_acc: 0.93333\n",
            "Epoch [520/10000], loss: 0.49604 acc: 1.00000 val_loss: 0.53116, val_acc: 0.93333\n",
            "Epoch [530/10000], loss: 0.49219 acc: 1.00000 val_loss: 0.52766, val_acc: 0.93333\n",
            "Epoch [540/10000], loss: 0.48841 acc: 1.00000 val_loss: 0.52421, val_acc: 0.93333\n",
            "Epoch [550/10000], loss: 0.48468 acc: 1.00000 val_loss: 0.52080, val_acc: 0.93333\n",
            "Epoch [560/10000], loss: 0.48101 acc: 1.00000 val_loss: 0.51745, val_acc: 0.93333\n",
            "Epoch [570/10000], loss: 0.47740 acc: 1.00000 val_loss: 0.51415, val_acc: 0.93333\n",
            "Epoch [580/10000], loss: 0.47384 acc: 1.00000 val_loss: 0.51089, val_acc: 0.93333\n",
            "Epoch [590/10000], loss: 0.47033 acc: 1.00000 val_loss: 0.50769, val_acc: 0.93333\n",
            "Epoch [600/10000], loss: 0.46687 acc: 1.00000 val_loss: 0.50452, val_acc: 0.93333\n",
            "Epoch [610/10000], loss: 0.46347 acc: 1.00000 val_loss: 0.50141, val_acc: 0.93333\n",
            "Epoch [620/10000], loss: 0.46011 acc: 1.00000 val_loss: 0.49833, val_acc: 0.93333\n",
            "Epoch [630/10000], loss: 0.45680 acc: 1.00000 val_loss: 0.49530, val_acc: 0.93333\n",
            "Epoch [640/10000], loss: 0.45355 acc: 1.00000 val_loss: 0.49232, val_acc: 0.93333\n",
            "Epoch [650/10000], loss: 0.45033 acc: 1.00000 val_loss: 0.48937, val_acc: 0.93333\n",
            "Epoch [660/10000], loss: 0.44717 acc: 1.00000 val_loss: 0.48647, val_acc: 0.93333\n",
            "Epoch [670/10000], loss: 0.44405 acc: 1.00000 val_loss: 0.48360, val_acc: 0.93333\n",
            "Epoch [680/10000], loss: 0.44097 acc: 1.00000 val_loss: 0.48078, val_acc: 0.93333\n",
            "Epoch [690/10000], loss: 0.43794 acc: 1.00000 val_loss: 0.47800, val_acc: 0.93333\n",
            "Epoch [700/10000], loss: 0.43495 acc: 1.00000 val_loss: 0.47525, val_acc: 0.93333\n",
            "Epoch [710/10000], loss: 0.43200 acc: 1.00000 val_loss: 0.47254, val_acc: 0.93333\n",
            "Epoch [720/10000], loss: 0.42909 acc: 1.00000 val_loss: 0.46987, val_acc: 0.93333\n",
            "Epoch [730/10000], loss: 0.42623 acc: 1.00000 val_loss: 0.46723, val_acc: 0.93333\n",
            "Epoch [740/10000], loss: 0.42340 acc: 1.00000 val_loss: 0.46463, val_acc: 0.93333\n",
            "Epoch [750/10000], loss: 0.42061 acc: 1.00000 val_loss: 0.46206, val_acc: 0.93333\n",
            "Epoch [760/10000], loss: 0.41786 acc: 1.00000 val_loss: 0.45953, val_acc: 0.93333\n",
            "Epoch [770/10000], loss: 0.41515 acc: 1.00000 val_loss: 0.45703, val_acc: 0.93333\n",
            "Epoch [780/10000], loss: 0.41247 acc: 1.00000 val_loss: 0.45457, val_acc: 0.93333\n",
            "Epoch [790/10000], loss: 0.40983 acc: 1.00000 val_loss: 0.45213, val_acc: 0.93333\n",
            "Epoch [800/10000], loss: 0.40722 acc: 1.00000 val_loss: 0.44973, val_acc: 0.93333\n",
            "Epoch [810/10000], loss: 0.40465 acc: 1.00000 val_loss: 0.44736, val_acc: 0.93333\n",
            "Epoch [820/10000], loss: 0.40211 acc: 1.00000 val_loss: 0.44502, val_acc: 0.93333\n",
            "Epoch [830/10000], loss: 0.39961 acc: 1.00000 val_loss: 0.44271, val_acc: 0.93333\n",
            "Epoch [840/10000], loss: 0.39714 acc: 1.00000 val_loss: 0.44043, val_acc: 0.93333\n",
            "Epoch [850/10000], loss: 0.39470 acc: 1.00000 val_loss: 0.43818, val_acc: 0.93333\n",
            "Epoch [860/10000], loss: 0.39229 acc: 1.00000 val_loss: 0.43596, val_acc: 0.93333\n",
            "Epoch [870/10000], loss: 0.38992 acc: 1.00000 val_loss: 0.43377, val_acc: 0.93333\n",
            "Epoch [880/10000], loss: 0.38757 acc: 1.00000 val_loss: 0.43160, val_acc: 0.93333\n",
            "Epoch [890/10000], loss: 0.38525 acc: 1.00000 val_loss: 0.42946, val_acc: 0.96667\n",
            "Epoch [900/10000], loss: 0.38297 acc: 1.00000 val_loss: 0.42735, val_acc: 0.96667\n",
            "Epoch [910/10000], loss: 0.38071 acc: 1.00000 val_loss: 0.42526, val_acc: 0.96667\n",
            "Epoch [920/10000], loss: 0.37848 acc: 1.00000 val_loss: 0.42320, val_acc: 0.96667\n",
            "Epoch [930/10000], loss: 0.37628 acc: 1.00000 val_loss: 0.42116, val_acc: 0.96667\n",
            "Epoch [940/10000], loss: 0.37410 acc: 1.00000 val_loss: 0.41915, val_acc: 0.96667\n",
            "Epoch [950/10000], loss: 0.37196 acc: 1.00000 val_loss: 0.41717, val_acc: 0.96667\n",
            "Epoch [960/10000], loss: 0.36983 acc: 1.00000 val_loss: 0.41520, val_acc: 0.96667\n",
            "Epoch [970/10000], loss: 0.36774 acc: 1.00000 val_loss: 0.41327, val_acc: 0.96667\n",
            "Epoch [980/10000], loss: 0.36567 acc: 1.00000 val_loss: 0.41135, val_acc: 0.96667\n",
            "Epoch [990/10000], loss: 0.36362 acc: 1.00000 val_loss: 0.40946, val_acc: 0.96667\n",
            "Epoch [1000/10000], loss: 0.36160 acc: 1.00000 val_loss: 0.40759, val_acc: 0.96667\n",
            "Epoch [1010/10000], loss: 0.35961 acc: 1.00000 val_loss: 0.40574, val_acc: 0.96667\n",
            "Epoch [1020/10000], loss: 0.35763 acc: 1.00000 val_loss: 0.40391, val_acc: 0.96667\n",
            "Epoch [1030/10000], loss: 0.35568 acc: 1.00000 val_loss: 0.40211, val_acc: 0.96667\n",
            "Epoch [1040/10000], loss: 0.35376 acc: 1.00000 val_loss: 0.40032, val_acc: 0.96667\n",
            "Epoch [1050/10000], loss: 0.35186 acc: 1.00000 val_loss: 0.39856, val_acc: 0.96667\n",
            "Epoch [1060/10000], loss: 0.34997 acc: 1.00000 val_loss: 0.39682, val_acc: 0.96667\n",
            "Epoch [1070/10000], loss: 0.34811 acc: 1.00000 val_loss: 0.39509, val_acc: 0.96667\n",
            "Epoch [1080/10000], loss: 0.34628 acc: 1.00000 val_loss: 0.39339, val_acc: 0.96667\n",
            "Epoch [1090/10000], loss: 0.34446 acc: 1.00000 val_loss: 0.39171, val_acc: 0.96667\n",
            "Epoch [1100/10000], loss: 0.34266 acc: 1.00000 val_loss: 0.39004, val_acc: 0.96667\n",
            "Epoch [1110/10000], loss: 0.34089 acc: 1.00000 val_loss: 0.38839, val_acc: 0.96667\n",
            "Epoch [1120/10000], loss: 0.33913 acc: 1.00000 val_loss: 0.38677, val_acc: 0.96667\n",
            "Epoch [1130/10000], loss: 0.33739 acc: 1.00000 val_loss: 0.38516, val_acc: 0.96667\n",
            "Epoch [1140/10000], loss: 0.33568 acc: 1.00000 val_loss: 0.38357, val_acc: 0.96667\n",
            "Epoch [1150/10000], loss: 0.33398 acc: 1.00000 val_loss: 0.38199, val_acc: 0.96667\n",
            "Epoch [1160/10000], loss: 0.33230 acc: 1.00000 val_loss: 0.38043, val_acc: 0.96667\n",
            "Epoch [1170/10000], loss: 0.33064 acc: 1.00000 val_loss: 0.37889, val_acc: 0.96667\n",
            "Epoch [1180/10000], loss: 0.32900 acc: 1.00000 val_loss: 0.37737, val_acc: 0.96667\n",
            "Epoch [1190/10000], loss: 0.32737 acc: 1.00000 val_loss: 0.37586, val_acc: 0.96667\n",
            "Epoch [1200/10000], loss: 0.32577 acc: 1.00000 val_loss: 0.37437, val_acc: 0.96667\n",
            "Epoch [1210/10000], loss: 0.32418 acc: 1.00000 val_loss: 0.37290, val_acc: 0.96667\n",
            "Epoch [1220/10000], loss: 0.32260 acc: 1.00000 val_loss: 0.37144, val_acc: 0.96667\n",
            "Epoch [1230/10000], loss: 0.32105 acc: 1.00000 val_loss: 0.37000, val_acc: 0.96667\n",
            "Epoch [1240/10000], loss: 0.31951 acc: 1.00000 val_loss: 0.36857, val_acc: 0.96667\n",
            "Epoch [1250/10000], loss: 0.31799 acc: 1.00000 val_loss: 0.36716, val_acc: 0.96667\n",
            "Epoch [1260/10000], loss: 0.31648 acc: 1.00000 val_loss: 0.36576, val_acc: 0.96667\n",
            "Epoch [1270/10000], loss: 0.31499 acc: 1.00000 val_loss: 0.36437, val_acc: 0.96667\n",
            "Epoch [1280/10000], loss: 0.31351 acc: 1.00000 val_loss: 0.36301, val_acc: 0.96667\n",
            "Epoch [1290/10000], loss: 0.31205 acc: 1.00000 val_loss: 0.36165, val_acc: 0.96667\n",
            "Epoch [1300/10000], loss: 0.31061 acc: 1.00000 val_loss: 0.36031, val_acc: 0.96667\n",
            "Epoch [1310/10000], loss: 0.30918 acc: 1.00000 val_loss: 0.35898, val_acc: 0.96667\n",
            "Epoch [1320/10000], loss: 0.30776 acc: 1.00000 val_loss: 0.35767, val_acc: 0.96667\n",
            "Epoch [1330/10000], loss: 0.30636 acc: 1.00000 val_loss: 0.35637, val_acc: 0.96667\n",
            "Epoch [1340/10000], loss: 0.30498 acc: 1.00000 val_loss: 0.35508, val_acc: 0.96667\n",
            "Epoch [1350/10000], loss: 0.30360 acc: 1.00000 val_loss: 0.35381, val_acc: 0.96667\n",
            "Epoch [1360/10000], loss: 0.30224 acc: 1.00000 val_loss: 0.35255, val_acc: 0.96667\n",
            "Epoch [1370/10000], loss: 0.30090 acc: 1.00000 val_loss: 0.35130, val_acc: 0.96667\n",
            "Epoch [1380/10000], loss: 0.29957 acc: 1.00000 val_loss: 0.35006, val_acc: 0.96667\n",
            "Epoch [1390/10000], loss: 0.29825 acc: 1.00000 val_loss: 0.34884, val_acc: 0.96667\n",
            "Epoch [1400/10000], loss: 0.29694 acc: 1.00000 val_loss: 0.34763, val_acc: 0.96667\n",
            "Epoch [1410/10000], loss: 0.29565 acc: 1.00000 val_loss: 0.34643, val_acc: 0.96667\n",
            "Epoch [1420/10000], loss: 0.29437 acc: 1.00000 val_loss: 0.34524, val_acc: 0.96667\n",
            "Epoch [1430/10000], loss: 0.29310 acc: 1.00000 val_loss: 0.34406, val_acc: 0.96667\n",
            "Epoch [1440/10000], loss: 0.29184 acc: 1.00000 val_loss: 0.34290, val_acc: 0.96667\n",
            "Epoch [1450/10000], loss: 0.29060 acc: 1.00000 val_loss: 0.34174, val_acc: 0.96667\n",
            "Epoch [1460/10000], loss: 0.28937 acc: 1.00000 val_loss: 0.34060, val_acc: 0.96667\n",
            "Epoch [1470/10000], loss: 0.28815 acc: 1.00000 val_loss: 0.33947, val_acc: 0.96667\n",
            "Epoch [1480/10000], loss: 0.28694 acc: 1.00000 val_loss: 0.33834, val_acc: 0.96667\n",
            "Epoch [1490/10000], loss: 0.28574 acc: 1.00000 val_loss: 0.33723, val_acc: 0.96667\n",
            "Epoch [1500/10000], loss: 0.28456 acc: 1.00000 val_loss: 0.33613, val_acc: 0.96667\n",
            "Epoch [1510/10000], loss: 0.28338 acc: 1.00000 val_loss: 0.33504, val_acc: 0.96667\n",
            "Epoch [1520/10000], loss: 0.28222 acc: 1.00000 val_loss: 0.33396, val_acc: 0.96667\n",
            "Epoch [1530/10000], loss: 0.28106 acc: 1.00000 val_loss: 0.33289, val_acc: 0.96667\n",
            "Epoch [1540/10000], loss: 0.27992 acc: 1.00000 val_loss: 0.33183, val_acc: 0.96667\n",
            "Epoch [1550/10000], loss: 0.27879 acc: 1.00000 val_loss: 0.33078, val_acc: 0.96667\n",
            "Epoch [1560/10000], loss: 0.27767 acc: 1.00000 val_loss: 0.32974, val_acc: 0.96667\n",
            "Epoch [1570/10000], loss: 0.27656 acc: 1.00000 val_loss: 0.32871, val_acc: 0.96667\n",
            "Epoch [1580/10000], loss: 0.27545 acc: 1.00000 val_loss: 0.32769, val_acc: 0.96667\n",
            "Epoch [1590/10000], loss: 0.27436 acc: 1.00000 val_loss: 0.32668, val_acc: 0.96667\n",
            "Epoch [1600/10000], loss: 0.27328 acc: 1.00000 val_loss: 0.32568, val_acc: 0.96667\n",
            "Epoch [1610/10000], loss: 0.27221 acc: 1.00000 val_loss: 0.32468, val_acc: 0.96667\n",
            "Epoch [1620/10000], loss: 0.27115 acc: 1.00000 val_loss: 0.32370, val_acc: 0.96667\n",
            "Epoch [1630/10000], loss: 0.27009 acc: 1.00000 val_loss: 0.32272, val_acc: 0.96667\n",
            "Epoch [1640/10000], loss: 0.26905 acc: 1.00000 val_loss: 0.32175, val_acc: 0.96667\n",
            "Epoch [1650/10000], loss: 0.26802 acc: 1.00000 val_loss: 0.32080, val_acc: 0.96667\n",
            "Epoch [1660/10000], loss: 0.26699 acc: 1.00000 val_loss: 0.31985, val_acc: 0.96667\n",
            "Epoch [1670/10000], loss: 0.26597 acc: 1.00000 val_loss: 0.31890, val_acc: 0.96667\n",
            "Epoch [1680/10000], loss: 0.26497 acc: 1.00000 val_loss: 0.31797, val_acc: 0.96667\n",
            "Epoch [1690/10000], loss: 0.26397 acc: 1.00000 val_loss: 0.31704, val_acc: 0.96667\n",
            "Epoch [1700/10000], loss: 0.26298 acc: 1.00000 val_loss: 0.31613, val_acc: 0.96667\n",
            "Epoch [1710/10000], loss: 0.26199 acc: 1.00000 val_loss: 0.31522, val_acc: 0.96667\n",
            "Epoch [1720/10000], loss: 0.26102 acc: 1.00000 val_loss: 0.31431, val_acc: 0.96667\n",
            "Epoch [1730/10000], loss: 0.26005 acc: 1.00000 val_loss: 0.31342, val_acc: 0.96667\n",
            "Epoch [1740/10000], loss: 0.25910 acc: 1.00000 val_loss: 0.31253, val_acc: 0.96667\n",
            "Epoch [1750/10000], loss: 0.25815 acc: 1.00000 val_loss: 0.31165, val_acc: 0.96667\n",
            "Epoch [1760/10000], loss: 0.25721 acc: 1.00000 val_loss: 0.31078, val_acc: 0.96667\n",
            "Epoch [1770/10000], loss: 0.25627 acc: 1.00000 val_loss: 0.30992, val_acc: 0.96667\n",
            "Epoch [1780/10000], loss: 0.25535 acc: 1.00000 val_loss: 0.30906, val_acc: 0.96667\n",
            "Epoch [1790/10000], loss: 0.25443 acc: 1.00000 val_loss: 0.30821, val_acc: 0.96667\n",
            "Epoch [1800/10000], loss: 0.25352 acc: 1.00000 val_loss: 0.30737, val_acc: 0.96667\n",
            "Epoch [1810/10000], loss: 0.25262 acc: 1.00000 val_loss: 0.30653, val_acc: 0.96667\n",
            "Epoch [1820/10000], loss: 0.25172 acc: 1.00000 val_loss: 0.30571, val_acc: 0.96667\n",
            "Epoch [1830/10000], loss: 0.25083 acc: 1.00000 val_loss: 0.30488, val_acc: 0.96667\n",
            "Epoch [1840/10000], loss: 0.24995 acc: 1.00000 val_loss: 0.30407, val_acc: 0.96667\n",
            "Epoch [1850/10000], loss: 0.24908 acc: 1.00000 val_loss: 0.30326, val_acc: 0.96667\n",
            "Epoch [1860/10000], loss: 0.24821 acc: 1.00000 val_loss: 0.30246, val_acc: 0.96667\n",
            "Epoch [1870/10000], loss: 0.24735 acc: 1.00000 val_loss: 0.30166, val_acc: 0.96667\n",
            "Epoch [1880/10000], loss: 0.24650 acc: 1.00000 val_loss: 0.30088, val_acc: 0.96667\n",
            "Epoch [1890/10000], loss: 0.24565 acc: 1.00000 val_loss: 0.30009, val_acc: 0.96667\n",
            "Epoch [1900/10000], loss: 0.24481 acc: 1.00000 val_loss: 0.29932, val_acc: 0.96667\n",
            "Epoch [1910/10000], loss: 0.24398 acc: 1.00000 val_loss: 0.29855, val_acc: 0.96667\n",
            "Epoch [1920/10000], loss: 0.24315 acc: 1.00000 val_loss: 0.29778, val_acc: 0.96667\n",
            "Epoch [1930/10000], loss: 0.24233 acc: 1.00000 val_loss: 0.29702, val_acc: 0.96667\n",
            "Epoch [1940/10000], loss: 0.24152 acc: 1.00000 val_loss: 0.29627, val_acc: 0.96667\n",
            "Epoch [1950/10000], loss: 0.24071 acc: 1.00000 val_loss: 0.29553, val_acc: 0.96667\n",
            "Epoch [1960/10000], loss: 0.23991 acc: 1.00000 val_loss: 0.29479, val_acc: 0.96667\n",
            "Epoch [1970/10000], loss: 0.23911 acc: 1.00000 val_loss: 0.29405, val_acc: 0.96667\n",
            "Epoch [1980/10000], loss: 0.23833 acc: 1.00000 val_loss: 0.29332, val_acc: 0.96667\n",
            "Epoch [1990/10000], loss: 0.23754 acc: 1.00000 val_loss: 0.29260, val_acc: 0.96667\n",
            "Epoch [2000/10000], loss: 0.23677 acc: 1.00000 val_loss: 0.29188, val_acc: 0.96667\n",
            "Epoch [2010/10000], loss: 0.23599 acc: 1.00000 val_loss: 0.29117, val_acc: 0.96667\n",
            "Epoch [2020/10000], loss: 0.23523 acc: 1.00000 val_loss: 0.29047, val_acc: 0.96667\n",
            "Epoch [2030/10000], loss: 0.23447 acc: 1.00000 val_loss: 0.28977, val_acc: 0.96667\n",
            "Epoch [2040/10000], loss: 0.23372 acc: 1.00000 val_loss: 0.28907, val_acc: 0.96667\n",
            "Epoch [2050/10000], loss: 0.23297 acc: 1.00000 val_loss: 0.28838, val_acc: 0.96667\n",
            "Epoch [2060/10000], loss: 0.23223 acc: 1.00000 val_loss: 0.28770, val_acc: 0.96667\n",
            "Epoch [2070/10000], loss: 0.23149 acc: 1.00000 val_loss: 0.28702, val_acc: 0.96667\n",
            "Epoch [2080/10000], loss: 0.23076 acc: 1.00000 val_loss: 0.28634, val_acc: 0.96667\n",
            "Epoch [2090/10000], loss: 0.23003 acc: 1.00000 val_loss: 0.28567, val_acc: 0.96667\n",
            "Epoch [2100/10000], loss: 0.22931 acc: 1.00000 val_loss: 0.28501, val_acc: 0.96667\n",
            "Epoch [2110/10000], loss: 0.22859 acc: 1.00000 val_loss: 0.28435, val_acc: 0.96667\n",
            "Epoch [2120/10000], loss: 0.22788 acc: 1.00000 val_loss: 0.28369, val_acc: 0.96667\n",
            "Epoch [2130/10000], loss: 0.22718 acc: 1.00000 val_loss: 0.28304, val_acc: 0.96667\n",
            "Epoch [2140/10000], loss: 0.22648 acc: 1.00000 val_loss: 0.28240, val_acc: 0.96667\n",
            "Epoch [2150/10000], loss: 0.22578 acc: 1.00000 val_loss: 0.28176, val_acc: 0.96667\n",
            "Epoch [2160/10000], loss: 0.22509 acc: 1.00000 val_loss: 0.28112, val_acc: 0.96667\n",
            "Epoch [2170/10000], loss: 0.22441 acc: 1.00000 val_loss: 0.28049, val_acc: 0.96667\n",
            "Epoch [2180/10000], loss: 0.22373 acc: 1.00000 val_loss: 0.27986, val_acc: 0.96667\n",
            "Epoch [2190/10000], loss: 0.22305 acc: 1.00000 val_loss: 0.27924, val_acc: 0.96667\n",
            "Epoch [2200/10000], loss: 0.22238 acc: 1.00000 val_loss: 0.27862, val_acc: 0.96667\n",
            "Epoch [2210/10000], loss: 0.22171 acc: 1.00000 val_loss: 0.27801, val_acc: 0.96667\n",
            "Epoch [2220/10000], loss: 0.22105 acc: 1.00000 val_loss: 0.27740, val_acc: 0.96667\n",
            "Epoch [2230/10000], loss: 0.22039 acc: 1.00000 val_loss: 0.27680, val_acc: 0.96667\n",
            "Epoch [2240/10000], loss: 0.21974 acc: 1.00000 val_loss: 0.27620, val_acc: 0.96667\n",
            "Epoch [2250/10000], loss: 0.21909 acc: 1.00000 val_loss: 0.27560, val_acc: 0.96667\n",
            "Epoch [2260/10000], loss: 0.21845 acc: 1.00000 val_loss: 0.27501, val_acc: 0.96667\n",
            "Epoch [2270/10000], loss: 0.21781 acc: 1.00000 val_loss: 0.27442, val_acc: 0.96667\n",
            "Epoch [2280/10000], loss: 0.21718 acc: 1.00000 val_loss: 0.27384, val_acc: 0.96667\n",
            "Epoch [2290/10000], loss: 0.21655 acc: 1.00000 val_loss: 0.27326, val_acc: 0.96667\n",
            "Epoch [2300/10000], loss: 0.21592 acc: 1.00000 val_loss: 0.27269, val_acc: 0.96667\n",
            "Epoch [2310/10000], loss: 0.21530 acc: 1.00000 val_loss: 0.27211, val_acc: 0.96667\n",
            "Epoch [2320/10000], loss: 0.21468 acc: 1.00000 val_loss: 0.27155, val_acc: 0.96667\n",
            "Epoch [2330/10000], loss: 0.21407 acc: 1.00000 val_loss: 0.27098, val_acc: 0.96667\n",
            "Epoch [2340/10000], loss: 0.21346 acc: 1.00000 val_loss: 0.27042, val_acc: 0.96667\n",
            "Epoch [2350/10000], loss: 0.21285 acc: 1.00000 val_loss: 0.26987, val_acc: 0.96667\n",
            "Epoch [2360/10000], loss: 0.21225 acc: 1.00000 val_loss: 0.26932, val_acc: 0.96667\n",
            "Epoch [2370/10000], loss: 0.21165 acc: 1.00000 val_loss: 0.26877, val_acc: 0.96667\n",
            "Epoch [2380/10000], loss: 0.21106 acc: 1.00000 val_loss: 0.26822, val_acc: 0.96667\n",
            "Epoch [2390/10000], loss: 0.21047 acc: 1.00000 val_loss: 0.26768, val_acc: 0.96667\n",
            "Epoch [2400/10000], loss: 0.20988 acc: 1.00000 val_loss: 0.26715, val_acc: 0.96667\n",
            "Epoch [2410/10000], loss: 0.20930 acc: 1.00000 val_loss: 0.26661, val_acc: 0.96667\n",
            "Epoch [2420/10000], loss: 0.20872 acc: 1.00000 val_loss: 0.26608, val_acc: 0.96667\n",
            "Epoch [2430/10000], loss: 0.20815 acc: 1.00000 val_loss: 0.26556, val_acc: 0.96667\n",
            "Epoch [2440/10000], loss: 0.20758 acc: 1.00000 val_loss: 0.26503, val_acc: 0.96667\n",
            "Epoch [2450/10000], loss: 0.20701 acc: 1.00000 val_loss: 0.26451, val_acc: 0.96667\n",
            "Epoch [2460/10000], loss: 0.20645 acc: 1.00000 val_loss: 0.26400, val_acc: 0.96667\n",
            "Epoch [2470/10000], loss: 0.20589 acc: 1.00000 val_loss: 0.26349, val_acc: 0.96667\n",
            "Epoch [2480/10000], loss: 0.20533 acc: 1.00000 val_loss: 0.26298, val_acc: 0.96667\n",
            "Epoch [2490/10000], loss: 0.20478 acc: 1.00000 val_loss: 0.26247, val_acc: 0.96667\n",
            "Epoch [2500/10000], loss: 0.20423 acc: 1.00000 val_loss: 0.26197, val_acc: 0.96667\n",
            "Epoch [2510/10000], loss: 0.20369 acc: 1.00000 val_loss: 0.26147, val_acc: 0.96667\n",
            "Epoch [2520/10000], loss: 0.20314 acc: 1.00000 val_loss: 0.26097, val_acc: 0.96667\n",
            "Epoch [2530/10000], loss: 0.20261 acc: 1.00000 val_loss: 0.26048, val_acc: 0.96667\n",
            "Epoch [2540/10000], loss: 0.20207 acc: 1.00000 val_loss: 0.25999, val_acc: 0.96667\n",
            "Epoch [2550/10000], loss: 0.20154 acc: 1.00000 val_loss: 0.25950, val_acc: 0.96667\n",
            "Epoch [2560/10000], loss: 0.20101 acc: 1.00000 val_loss: 0.25902, val_acc: 0.96667\n",
            "Epoch [2570/10000], loss: 0.20048 acc: 1.00000 val_loss: 0.25854, val_acc: 0.96667\n",
            "Epoch [2580/10000], loss: 0.19996 acc: 1.00000 val_loss: 0.25806, val_acc: 0.96667\n",
            "Epoch [2590/10000], loss: 0.19944 acc: 1.00000 val_loss: 0.25759, val_acc: 0.96667\n",
            "Epoch [2600/10000], loss: 0.19893 acc: 1.00000 val_loss: 0.25712, val_acc: 0.96667\n",
            "Epoch [2610/10000], loss: 0.19841 acc: 1.00000 val_loss: 0.25665, val_acc: 0.96667\n",
            "Epoch [2620/10000], loss: 0.19791 acc: 1.00000 val_loss: 0.25618, val_acc: 0.96667\n",
            "Epoch [2630/10000], loss: 0.19740 acc: 1.00000 val_loss: 0.25572, val_acc: 0.96667\n",
            "Epoch [2640/10000], loss: 0.19690 acc: 1.00000 val_loss: 0.25526, val_acc: 0.96667\n",
            "Epoch [2650/10000], loss: 0.19640 acc: 1.00000 val_loss: 0.25481, val_acc: 0.96667\n",
            "Epoch [2660/10000], loss: 0.19590 acc: 1.00000 val_loss: 0.25435, val_acc: 0.96667\n",
            "Epoch [2670/10000], loss: 0.19540 acc: 1.00000 val_loss: 0.25390, val_acc: 0.96667\n",
            "Epoch [2680/10000], loss: 0.19491 acc: 1.00000 val_loss: 0.25345, val_acc: 0.96667\n",
            "Epoch [2690/10000], loss: 0.19442 acc: 1.00000 val_loss: 0.25301, val_acc: 0.96667\n",
            "Epoch [2700/10000], loss: 0.19394 acc: 1.00000 val_loss: 0.25257, val_acc: 0.96667\n",
            "Epoch [2710/10000], loss: 0.19346 acc: 1.00000 val_loss: 0.25213, val_acc: 0.96667\n",
            "Epoch [2720/10000], loss: 0.19298 acc: 1.00000 val_loss: 0.25169, val_acc: 0.96667\n",
            "Epoch [2730/10000], loss: 0.19250 acc: 1.00000 val_loss: 0.25125, val_acc: 0.96667\n",
            "Epoch [2740/10000], loss: 0.19202 acc: 1.00000 val_loss: 0.25082, val_acc: 0.96667\n",
            "Epoch [2750/10000], loss: 0.19155 acc: 1.00000 val_loss: 0.25039, val_acc: 0.96667\n",
            "Epoch [2760/10000], loss: 0.19108 acc: 1.00000 val_loss: 0.24997, val_acc: 0.96667\n",
            "Epoch [2770/10000], loss: 0.19062 acc: 1.00000 val_loss: 0.24954, val_acc: 0.96667\n",
            "Epoch [2780/10000], loss: 0.19016 acc: 1.00000 val_loss: 0.24912, val_acc: 0.96667\n",
            "Epoch [2790/10000], loss: 0.18970 acc: 1.00000 val_loss: 0.24870, val_acc: 0.96667\n",
            "Epoch [2800/10000], loss: 0.18924 acc: 1.00000 val_loss: 0.24828, val_acc: 0.96667\n",
            "Epoch [2810/10000], loss: 0.18878 acc: 1.00000 val_loss: 0.24787, val_acc: 0.96667\n",
            "Epoch [2820/10000], loss: 0.18833 acc: 1.00000 val_loss: 0.24746, val_acc: 0.96667\n",
            "Epoch [2830/10000], loss: 0.18788 acc: 1.00000 val_loss: 0.24705, val_acc: 0.96667\n",
            "Epoch [2840/10000], loss: 0.18743 acc: 1.00000 val_loss: 0.24664, val_acc: 0.96667\n",
            "Epoch [2850/10000], loss: 0.18699 acc: 1.00000 val_loss: 0.24624, val_acc: 0.96667\n",
            "Epoch [2860/10000], loss: 0.18654 acc: 1.00000 val_loss: 0.24584, val_acc: 0.96667\n",
            "Epoch [2870/10000], loss: 0.18610 acc: 1.00000 val_loss: 0.24544, val_acc: 0.96667\n",
            "Epoch [2880/10000], loss: 0.18567 acc: 1.00000 val_loss: 0.24504, val_acc: 0.96667\n",
            "Epoch [2890/10000], loss: 0.18523 acc: 1.00000 val_loss: 0.24464, val_acc: 0.96667\n",
            "Epoch [2900/10000], loss: 0.18480 acc: 1.00000 val_loss: 0.24425, val_acc: 0.96667\n",
            "Epoch [2910/10000], loss: 0.18437 acc: 1.00000 val_loss: 0.24386, val_acc: 0.96667\n",
            "Epoch [2920/10000], loss: 0.18394 acc: 1.00000 val_loss: 0.24347, val_acc: 0.96667\n",
            "Epoch [2930/10000], loss: 0.18352 acc: 1.00000 val_loss: 0.24309, val_acc: 0.96667\n",
            "Epoch [2940/10000], loss: 0.18309 acc: 1.00000 val_loss: 0.24270, val_acc: 0.96667\n",
            "Epoch [2950/10000], loss: 0.18267 acc: 1.00000 val_loss: 0.24232, val_acc: 0.96667\n",
            "Epoch [2960/10000], loss: 0.18225 acc: 1.00000 val_loss: 0.24194, val_acc: 0.96667\n",
            "Epoch [2970/10000], loss: 0.18184 acc: 1.00000 val_loss: 0.24156, val_acc: 0.96667\n",
            "Epoch [2980/10000], loss: 0.18142 acc: 1.00000 val_loss: 0.24119, val_acc: 0.96667\n",
            "Epoch [2990/10000], loss: 0.18101 acc: 1.00000 val_loss: 0.24081, val_acc: 0.96667\n",
            "Epoch [3000/10000], loss: 0.18060 acc: 1.00000 val_loss: 0.24044, val_acc: 0.96667\n",
            "Epoch [3010/10000], loss: 0.18019 acc: 1.00000 val_loss: 0.24008, val_acc: 0.96667\n",
            "Epoch [3020/10000], loss: 0.17979 acc: 1.00000 val_loss: 0.23971, val_acc: 0.96667\n",
            "Epoch [3030/10000], loss: 0.17939 acc: 1.00000 val_loss: 0.23934, val_acc: 0.96667\n",
            "Epoch [3040/10000], loss: 0.17899 acc: 1.00000 val_loss: 0.23898, val_acc: 0.96667\n",
            "Epoch [3050/10000], loss: 0.17859 acc: 1.00000 val_loss: 0.23862, val_acc: 0.96667\n",
            "Epoch [3060/10000], loss: 0.17819 acc: 1.00000 val_loss: 0.23826, val_acc: 0.96667\n",
            "Epoch [3070/10000], loss: 0.17780 acc: 1.00000 val_loss: 0.23790, val_acc: 0.96667\n",
            "Epoch [3080/10000], loss: 0.17740 acc: 1.00000 val_loss: 0.23755, val_acc: 0.96667\n",
            "Epoch [3090/10000], loss: 0.17701 acc: 1.00000 val_loss: 0.23720, val_acc: 0.96667\n",
            "Epoch [3100/10000], loss: 0.17662 acc: 1.00000 val_loss: 0.23685, val_acc: 0.96667\n",
            "Epoch [3110/10000], loss: 0.17624 acc: 1.00000 val_loss: 0.23650, val_acc: 0.96667\n",
            "Epoch [3120/10000], loss: 0.17585 acc: 1.00000 val_loss: 0.23615, val_acc: 0.96667\n",
            "Epoch [3130/10000], loss: 0.17547 acc: 1.00000 val_loss: 0.23580, val_acc: 0.96667\n",
            "Epoch [3140/10000], loss: 0.17509 acc: 1.00000 val_loss: 0.23546, val_acc: 0.96667\n",
            "Epoch [3150/10000], loss: 0.17471 acc: 1.00000 val_loss: 0.23512, val_acc: 0.96667\n",
            "Epoch [3160/10000], loss: 0.17434 acc: 1.00000 val_loss: 0.23478, val_acc: 0.96667\n",
            "Epoch [3170/10000], loss: 0.17396 acc: 1.00000 val_loss: 0.23444, val_acc: 0.96667\n",
            "Epoch [3180/10000], loss: 0.17359 acc: 1.00000 val_loss: 0.23411, val_acc: 0.96667\n",
            "Epoch [3190/10000], loss: 0.17322 acc: 1.00000 val_loss: 0.23377, val_acc: 0.96667\n",
            "Epoch [3200/10000], loss: 0.17285 acc: 1.00000 val_loss: 0.23344, val_acc: 0.96667\n",
            "Epoch [3210/10000], loss: 0.17249 acc: 1.00000 val_loss: 0.23311, val_acc: 0.96667\n",
            "Epoch [3220/10000], loss: 0.17212 acc: 1.00000 val_loss: 0.23278, val_acc: 0.96667\n",
            "Epoch [3230/10000], loss: 0.17176 acc: 1.00000 val_loss: 0.23245, val_acc: 0.96667\n",
            "Epoch [3240/10000], loss: 0.17140 acc: 1.00000 val_loss: 0.23213, val_acc: 0.96667\n",
            "Epoch [3250/10000], loss: 0.17104 acc: 1.00000 val_loss: 0.23180, val_acc: 0.96667\n",
            "Epoch [3260/10000], loss: 0.17068 acc: 1.00000 val_loss: 0.23148, val_acc: 0.96667\n",
            "Epoch [3270/10000], loss: 0.17032 acc: 1.00000 val_loss: 0.23116, val_acc: 0.96667\n",
            "Epoch [3280/10000], loss: 0.16997 acc: 1.00000 val_loss: 0.23084, val_acc: 0.96667\n",
            "Epoch [3290/10000], loss: 0.16962 acc: 1.00000 val_loss: 0.23053, val_acc: 0.96667\n",
            "Epoch [3300/10000], loss: 0.16927 acc: 1.00000 val_loss: 0.23021, val_acc: 0.96667\n",
            "Epoch [3310/10000], loss: 0.16892 acc: 1.00000 val_loss: 0.22990, val_acc: 0.96667\n",
            "Epoch [3320/10000], loss: 0.16857 acc: 1.00000 val_loss: 0.22959, val_acc: 0.96667\n",
            "Epoch [3330/10000], loss: 0.16823 acc: 1.00000 val_loss: 0.22928, val_acc: 0.96667\n",
            "Epoch [3340/10000], loss: 0.16788 acc: 1.00000 val_loss: 0.22897, val_acc: 0.96667\n",
            "Epoch [3350/10000], loss: 0.16754 acc: 1.00000 val_loss: 0.22866, val_acc: 0.96667\n",
            "Epoch [3360/10000], loss: 0.16720 acc: 1.00000 val_loss: 0.22835, val_acc: 0.96667\n",
            "Epoch [3370/10000], loss: 0.16686 acc: 1.00000 val_loss: 0.22805, val_acc: 0.96667\n",
            "Epoch [3380/10000], loss: 0.16653 acc: 1.00000 val_loss: 0.22775, val_acc: 0.96667\n",
            "Epoch [3390/10000], loss: 0.16619 acc: 1.00000 val_loss: 0.22745, val_acc: 0.96667\n",
            "Epoch [3400/10000], loss: 0.16586 acc: 1.00000 val_loss: 0.22715, val_acc: 0.96667\n",
            "Epoch [3410/10000], loss: 0.16553 acc: 1.00000 val_loss: 0.22685, val_acc: 0.96667\n",
            "Epoch [3420/10000], loss: 0.16520 acc: 1.00000 val_loss: 0.22655, val_acc: 0.96667\n",
            "Epoch [3430/10000], loss: 0.16487 acc: 1.00000 val_loss: 0.22626, val_acc: 0.96667\n",
            "Epoch [3440/10000], loss: 0.16454 acc: 1.00000 val_loss: 0.22596, val_acc: 0.96667\n",
            "Epoch [3450/10000], loss: 0.16421 acc: 1.00000 val_loss: 0.22567, val_acc: 0.96667\n",
            "Epoch [3460/10000], loss: 0.16389 acc: 1.00000 val_loss: 0.22538, val_acc: 0.96667\n",
            "Epoch [3470/10000], loss: 0.16357 acc: 1.00000 val_loss: 0.22509, val_acc: 0.96667\n",
            "Epoch [3480/10000], loss: 0.16325 acc: 1.00000 val_loss: 0.22480, val_acc: 0.96667\n",
            "Epoch [3490/10000], loss: 0.16293 acc: 1.00000 val_loss: 0.22452, val_acc: 0.96667\n",
            "Epoch [3500/10000], loss: 0.16261 acc: 1.00000 val_loss: 0.22423, val_acc: 0.96667\n",
            "Epoch [3510/10000], loss: 0.16229 acc: 1.00000 val_loss: 0.22395, val_acc: 0.96667\n",
            "Epoch [3520/10000], loss: 0.16198 acc: 1.00000 val_loss: 0.22367, val_acc: 0.96667\n",
            "Epoch [3530/10000], loss: 0.16166 acc: 1.00000 val_loss: 0.22339, val_acc: 0.96667\n",
            "Epoch [3540/10000], loss: 0.16135 acc: 1.00000 val_loss: 0.22311, val_acc: 0.96667\n",
            "Epoch [3550/10000], loss: 0.16104 acc: 1.00000 val_loss: 0.22283, val_acc: 0.96667\n",
            "Epoch [3560/10000], loss: 0.16073 acc: 1.00000 val_loss: 0.22255, val_acc: 0.96667\n",
            "Epoch [3570/10000], loss: 0.16043 acc: 1.00000 val_loss: 0.22228, val_acc: 0.96667\n",
            "Epoch [3580/10000], loss: 0.16012 acc: 1.00000 val_loss: 0.22200, val_acc: 0.96667\n",
            "Epoch [3590/10000], loss: 0.15981 acc: 1.00000 val_loss: 0.22173, val_acc: 0.96667\n",
            "Epoch [3600/10000], loss: 0.15951 acc: 1.00000 val_loss: 0.22146, val_acc: 0.96667\n",
            "Epoch [3610/10000], loss: 0.15921 acc: 1.00000 val_loss: 0.22119, val_acc: 0.96667\n",
            "Epoch [3620/10000], loss: 0.15891 acc: 1.00000 val_loss: 0.22092, val_acc: 0.96667\n",
            "Epoch [3630/10000], loss: 0.15861 acc: 1.00000 val_loss: 0.22065, val_acc: 0.96667\n",
            "Epoch [3640/10000], loss: 0.15831 acc: 1.00000 val_loss: 0.22039, val_acc: 0.96667\n",
            "Epoch [3650/10000], loss: 0.15801 acc: 1.00000 val_loss: 0.22012, val_acc: 0.96667\n",
            "Epoch [3660/10000], loss: 0.15772 acc: 1.00000 val_loss: 0.21986, val_acc: 0.96667\n",
            "Epoch [3670/10000], loss: 0.15743 acc: 1.00000 val_loss: 0.21960, val_acc: 0.96667\n",
            "Epoch [3680/10000], loss: 0.15713 acc: 1.00000 val_loss: 0.21934, val_acc: 0.96667\n",
            "Epoch [3690/10000], loss: 0.15684 acc: 1.00000 val_loss: 0.21908, val_acc: 0.96667\n",
            "Epoch [3700/10000], loss: 0.15655 acc: 1.00000 val_loss: 0.21882, val_acc: 0.96667\n",
            "Epoch [3710/10000], loss: 0.15626 acc: 1.00000 val_loss: 0.21856, val_acc: 0.96667\n",
            "Epoch [3720/10000], loss: 0.15598 acc: 1.00000 val_loss: 0.21830, val_acc: 0.96667\n",
            "Epoch [3730/10000], loss: 0.15569 acc: 1.00000 val_loss: 0.21805, val_acc: 0.96667\n",
            "Epoch [3740/10000], loss: 0.15540 acc: 1.00000 val_loss: 0.21780, val_acc: 0.96667\n",
            "Epoch [3750/10000], loss: 0.15512 acc: 1.00000 val_loss: 0.21754, val_acc: 0.96667\n",
            "Epoch [3760/10000], loss: 0.15484 acc: 1.00000 val_loss: 0.21729, val_acc: 0.96667\n",
            "Epoch [3770/10000], loss: 0.15456 acc: 1.00000 val_loss: 0.21704, val_acc: 0.96667\n",
            "Epoch [3780/10000], loss: 0.15428 acc: 1.00000 val_loss: 0.21679, val_acc: 0.96667\n",
            "Epoch [3790/10000], loss: 0.15400 acc: 1.00000 val_loss: 0.21655, val_acc: 0.96667\n",
            "Epoch [3800/10000], loss: 0.15372 acc: 1.00000 val_loss: 0.21630, val_acc: 0.96667\n",
            "Epoch [3810/10000], loss: 0.15345 acc: 1.00000 val_loss: 0.21605, val_acc: 0.96667\n",
            "Epoch [3820/10000], loss: 0.15317 acc: 1.00000 val_loss: 0.21581, val_acc: 0.96667\n",
            "Epoch [3830/10000], loss: 0.15290 acc: 1.00000 val_loss: 0.21557, val_acc: 0.96667\n",
            "Epoch [3840/10000], loss: 0.15262 acc: 1.00000 val_loss: 0.21532, val_acc: 0.96667\n",
            "Epoch [3850/10000], loss: 0.15235 acc: 1.00000 val_loss: 0.21508, val_acc: 0.96667\n",
            "Epoch [3860/10000], loss: 0.15208 acc: 1.00000 val_loss: 0.21484, val_acc: 0.96667\n",
            "Epoch [3870/10000], loss: 0.15181 acc: 1.00000 val_loss: 0.21460, val_acc: 0.96667\n",
            "Epoch [3880/10000], loss: 0.15155 acc: 1.00000 val_loss: 0.21436, val_acc: 0.96667\n",
            "Epoch [3890/10000], loss: 0.15128 acc: 1.00000 val_loss: 0.21413, val_acc: 0.96667\n",
            "Epoch [3900/10000], loss: 0.15101 acc: 1.00000 val_loss: 0.21389, val_acc: 0.96667\n",
            "Epoch [3910/10000], loss: 0.15075 acc: 1.00000 val_loss: 0.21366, val_acc: 0.96667\n",
            "Epoch [3920/10000], loss: 0.15049 acc: 1.00000 val_loss: 0.21342, val_acc: 0.96667\n",
            "Epoch [3930/10000], loss: 0.15022 acc: 1.00000 val_loss: 0.21319, val_acc: 0.96667\n",
            "Epoch [3940/10000], loss: 0.14996 acc: 1.00000 val_loss: 0.21296, val_acc: 0.96667\n",
            "Epoch [3950/10000], loss: 0.14970 acc: 1.00000 val_loss: 0.21273, val_acc: 0.96667\n",
            "Epoch [3960/10000], loss: 0.14944 acc: 1.00000 val_loss: 0.21250, val_acc: 0.96667\n",
            "Epoch [3970/10000], loss: 0.14919 acc: 1.00000 val_loss: 0.21227, val_acc: 0.96667\n",
            "Epoch [3980/10000], loss: 0.14893 acc: 1.00000 val_loss: 0.21204, val_acc: 0.96667\n",
            "Epoch [3990/10000], loss: 0.14867 acc: 1.00000 val_loss: 0.21182, val_acc: 0.96667\n",
            "Epoch [4000/10000], loss: 0.14842 acc: 1.00000 val_loss: 0.21159, val_acc: 0.96667\n",
            "Epoch [4010/10000], loss: 0.14816 acc: 1.00000 val_loss: 0.21137, val_acc: 0.96667\n",
            "Epoch [4020/10000], loss: 0.14791 acc: 1.00000 val_loss: 0.21114, val_acc: 0.96667\n",
            "Epoch [4030/10000], loss: 0.14766 acc: 1.00000 val_loss: 0.21092, val_acc: 0.96667\n",
            "Epoch [4040/10000], loss: 0.14741 acc: 1.00000 val_loss: 0.21070, val_acc: 0.96667\n",
            "Epoch [4050/10000], loss: 0.14716 acc: 1.00000 val_loss: 0.21048, val_acc: 0.96667\n",
            "Epoch [4060/10000], loss: 0.14691 acc: 1.00000 val_loss: 0.21026, val_acc: 0.96667\n",
            "Epoch [4070/10000], loss: 0.14667 acc: 1.00000 val_loss: 0.21004, val_acc: 0.96667\n",
            "Epoch [4080/10000], loss: 0.14642 acc: 1.00000 val_loss: 0.20982, val_acc: 0.96667\n",
            "Epoch [4090/10000], loss: 0.14617 acc: 1.00000 val_loss: 0.20961, val_acc: 0.96667\n",
            "Epoch [4100/10000], loss: 0.14593 acc: 1.00000 val_loss: 0.20939, val_acc: 0.96667\n",
            "Epoch [4110/10000], loss: 0.14569 acc: 1.00000 val_loss: 0.20918, val_acc: 0.96667\n",
            "Epoch [4120/10000], loss: 0.14544 acc: 1.00000 val_loss: 0.20896, val_acc: 0.96667\n",
            "Epoch [4130/10000], loss: 0.14520 acc: 1.00000 val_loss: 0.20875, val_acc: 0.96667\n",
            "Epoch [4140/10000], loss: 0.14496 acc: 1.00000 val_loss: 0.20854, val_acc: 0.96667\n",
            "Epoch [4150/10000], loss: 0.14472 acc: 1.00000 val_loss: 0.20833, val_acc: 0.96667\n",
            "Epoch [4160/10000], loss: 0.14448 acc: 1.00000 val_loss: 0.20812, val_acc: 0.96667\n",
            "Epoch [4170/10000], loss: 0.14425 acc: 1.00000 val_loss: 0.20791, val_acc: 0.96667\n",
            "Epoch [4180/10000], loss: 0.14401 acc: 1.00000 val_loss: 0.20770, val_acc: 0.96667\n",
            "Epoch [4190/10000], loss: 0.14377 acc: 1.00000 val_loss: 0.20749, val_acc: 0.96667\n",
            "Epoch [4200/10000], loss: 0.14354 acc: 1.00000 val_loss: 0.20728, val_acc: 0.96667\n",
            "Epoch [4210/10000], loss: 0.14331 acc: 1.00000 val_loss: 0.20708, val_acc: 0.96667\n",
            "Epoch [4220/10000], loss: 0.14307 acc: 1.00000 val_loss: 0.20687, val_acc: 0.96667\n",
            "Epoch [4230/10000], loss: 0.14284 acc: 1.00000 val_loss: 0.20667, val_acc: 0.96667\n",
            "Epoch [4240/10000], loss: 0.14261 acc: 1.00000 val_loss: 0.20647, val_acc: 0.96667\n",
            "Epoch [4250/10000], loss: 0.14238 acc: 1.00000 val_loss: 0.20626, val_acc: 0.96667\n",
            "Epoch [4260/10000], loss: 0.14215 acc: 1.00000 val_loss: 0.20606, val_acc: 0.96667\n",
            "Epoch [4270/10000], loss: 0.14192 acc: 1.00000 val_loss: 0.20586, val_acc: 0.96667\n",
            "Epoch [4280/10000], loss: 0.14170 acc: 1.00000 val_loss: 0.20566, val_acc: 0.96667\n",
            "Epoch [4290/10000], loss: 0.14147 acc: 1.00000 val_loss: 0.20546, val_acc: 0.96667\n",
            "Epoch [4300/10000], loss: 0.14124 acc: 1.00000 val_loss: 0.20526, val_acc: 0.96667\n",
            "Epoch [4310/10000], loss: 0.14102 acc: 1.00000 val_loss: 0.20507, val_acc: 0.96667\n",
            "Epoch [4320/10000], loss: 0.14080 acc: 1.00000 val_loss: 0.20487, val_acc: 0.96667\n",
            "Epoch [4330/10000], loss: 0.14057 acc: 1.00000 val_loss: 0.20467, val_acc: 0.96667\n",
            "Epoch [4340/10000], loss: 0.14035 acc: 1.00000 val_loss: 0.20448, val_acc: 0.96667\n",
            "Epoch [4350/10000], loss: 0.14013 acc: 1.00000 val_loss: 0.20429, val_acc: 0.96667\n",
            "Epoch [4360/10000], loss: 0.13991 acc: 1.00000 val_loss: 0.20409, val_acc: 0.96667\n",
            "Epoch [4370/10000], loss: 0.13969 acc: 1.00000 val_loss: 0.20390, val_acc: 0.96667\n",
            "Epoch [4380/10000], loss: 0.13947 acc: 1.00000 val_loss: 0.20371, val_acc: 0.96667\n",
            "Epoch [4390/10000], loss: 0.13925 acc: 1.00000 val_loss: 0.20352, val_acc: 0.96667\n",
            "Epoch [4400/10000], loss: 0.13904 acc: 1.00000 val_loss: 0.20333, val_acc: 0.96667\n",
            "Epoch [4410/10000], loss: 0.13882 acc: 1.00000 val_loss: 0.20314, val_acc: 0.96667\n",
            "Epoch [4420/10000], loss: 0.13860 acc: 1.00000 val_loss: 0.20295, val_acc: 0.96667\n",
            "Epoch [4430/10000], loss: 0.13839 acc: 1.00000 val_loss: 0.20276, val_acc: 0.96667\n",
            "Epoch [4440/10000], loss: 0.13818 acc: 1.00000 val_loss: 0.20257, val_acc: 0.96667\n",
            "Epoch [4450/10000], loss: 0.13796 acc: 1.00000 val_loss: 0.20239, val_acc: 0.96667\n",
            "Epoch [4460/10000], loss: 0.13775 acc: 1.00000 val_loss: 0.20220, val_acc: 0.96667\n",
            "Epoch [4470/10000], loss: 0.13754 acc: 1.00000 val_loss: 0.20202, val_acc: 0.96667\n",
            "Epoch [4480/10000], loss: 0.13733 acc: 1.00000 val_loss: 0.20183, val_acc: 0.96667\n",
            "Epoch [4490/10000], loss: 0.13712 acc: 1.00000 val_loss: 0.20165, val_acc: 0.96667\n",
            "Epoch [4500/10000], loss: 0.13691 acc: 1.00000 val_loss: 0.20147, val_acc: 0.96667\n",
            "Epoch [4510/10000], loss: 0.13670 acc: 1.00000 val_loss: 0.20128, val_acc: 0.96667\n",
            "Epoch [4520/10000], loss: 0.13650 acc: 1.00000 val_loss: 0.20110, val_acc: 0.96667\n",
            "Epoch [4530/10000], loss: 0.13629 acc: 1.00000 val_loss: 0.20092, val_acc: 0.96667\n",
            "Epoch [4540/10000], loss: 0.13608 acc: 1.00000 val_loss: 0.20074, val_acc: 0.96667\n",
            "Epoch [4550/10000], loss: 0.13588 acc: 1.00000 val_loss: 0.20056, val_acc: 0.96667\n",
            "Epoch [4560/10000], loss: 0.13567 acc: 1.00000 val_loss: 0.20038, val_acc: 0.96667\n",
            "Epoch [4570/10000], loss: 0.13547 acc: 1.00000 val_loss: 0.20021, val_acc: 0.96667\n",
            "Epoch [4580/10000], loss: 0.13527 acc: 1.00000 val_loss: 0.20003, val_acc: 0.96667\n",
            "Epoch [4590/10000], loss: 0.13507 acc: 1.00000 val_loss: 0.19985, val_acc: 0.96667\n",
            "Epoch [4600/10000], loss: 0.13486 acc: 1.00000 val_loss: 0.19968, val_acc: 0.96667\n",
            "Epoch [4610/10000], loss: 0.13466 acc: 1.00000 val_loss: 0.19950, val_acc: 0.96667\n",
            "Epoch [4620/10000], loss: 0.13446 acc: 1.00000 val_loss: 0.19933, val_acc: 0.96667\n",
            "Epoch [4630/10000], loss: 0.13426 acc: 1.00000 val_loss: 0.19916, val_acc: 0.96667\n",
            "Epoch [4640/10000], loss: 0.13407 acc: 1.00000 val_loss: 0.19898, val_acc: 0.96667\n",
            "Epoch [4650/10000], loss: 0.13387 acc: 1.00000 val_loss: 0.19881, val_acc: 0.96667\n",
            "Epoch [4660/10000], loss: 0.13367 acc: 1.00000 val_loss: 0.19864, val_acc: 0.96667\n",
            "Epoch [4670/10000], loss: 0.13348 acc: 1.00000 val_loss: 0.19847, val_acc: 0.96667\n",
            "Epoch [4680/10000], loss: 0.13328 acc: 1.00000 val_loss: 0.19830, val_acc: 0.96667\n",
            "Epoch [4690/10000], loss: 0.13308 acc: 1.00000 val_loss: 0.19813, val_acc: 0.96667\n",
            "Epoch [4700/10000], loss: 0.13289 acc: 1.00000 val_loss: 0.19796, val_acc: 0.96667\n",
            "Epoch [4710/10000], loss: 0.13270 acc: 1.00000 val_loss: 0.19779, val_acc: 0.96667\n",
            "Epoch [4720/10000], loss: 0.13250 acc: 1.00000 val_loss: 0.19762, val_acc: 0.96667\n",
            "Epoch [4730/10000], loss: 0.13231 acc: 1.00000 val_loss: 0.19746, val_acc: 0.96667\n",
            "Epoch [4740/10000], loss: 0.13212 acc: 1.00000 val_loss: 0.19729, val_acc: 0.96667\n",
            "Epoch [4750/10000], loss: 0.13193 acc: 1.00000 val_loss: 0.19712, val_acc: 0.96667\n",
            "Epoch [4760/10000], loss: 0.13174 acc: 1.00000 val_loss: 0.19696, val_acc: 0.96667\n",
            "Epoch [4770/10000], loss: 0.13155 acc: 1.00000 val_loss: 0.19679, val_acc: 0.96667\n",
            "Epoch [4780/10000], loss: 0.13136 acc: 1.00000 val_loss: 0.19663, val_acc: 0.96667\n",
            "Epoch [4790/10000], loss: 0.13117 acc: 1.00000 val_loss: 0.19647, val_acc: 0.96667\n",
            "Epoch [4800/10000], loss: 0.13099 acc: 1.00000 val_loss: 0.19630, val_acc: 0.96667\n",
            "Epoch [4810/10000], loss: 0.13080 acc: 1.00000 val_loss: 0.19614, val_acc: 0.96667\n",
            "Epoch [4820/10000], loss: 0.13061 acc: 1.00000 val_loss: 0.19598, val_acc: 0.96667\n",
            "Epoch [4830/10000], loss: 0.13043 acc: 1.00000 val_loss: 0.19582, val_acc: 0.96667\n",
            "Epoch [4840/10000], loss: 0.13024 acc: 1.00000 val_loss: 0.19566, val_acc: 0.96667\n",
            "Epoch [4850/10000], loss: 0.13006 acc: 1.00000 val_loss: 0.19550, val_acc: 0.96667\n",
            "Epoch [4860/10000], loss: 0.12988 acc: 1.00000 val_loss: 0.19534, val_acc: 0.96667\n",
            "Epoch [4870/10000], loss: 0.12969 acc: 1.00000 val_loss: 0.19518, val_acc: 0.96667\n",
            "Epoch [4880/10000], loss: 0.12951 acc: 1.00000 val_loss: 0.19502, val_acc: 0.96667\n",
            "Epoch [4890/10000], loss: 0.12933 acc: 1.00000 val_loss: 0.19487, val_acc: 0.96667\n",
            "Epoch [4900/10000], loss: 0.12915 acc: 1.00000 val_loss: 0.19471, val_acc: 0.96667\n",
            "Epoch [4910/10000], loss: 0.12897 acc: 1.00000 val_loss: 0.19455, val_acc: 0.96667\n",
            "Epoch [4920/10000], loss: 0.12879 acc: 1.00000 val_loss: 0.19440, val_acc: 0.96667\n",
            "Epoch [4930/10000], loss: 0.12861 acc: 1.00000 val_loss: 0.19424, val_acc: 0.96667\n",
            "Epoch [4940/10000], loss: 0.12843 acc: 1.00000 val_loss: 0.19409, val_acc: 0.96667\n",
            "Epoch [4950/10000], loss: 0.12825 acc: 1.00000 val_loss: 0.19394, val_acc: 0.96667\n",
            "Epoch [4960/10000], loss: 0.12808 acc: 1.00000 val_loss: 0.19378, val_acc: 0.96667\n",
            "Epoch [4970/10000], loss: 0.12790 acc: 1.00000 val_loss: 0.19363, val_acc: 0.96667\n",
            "Epoch [4980/10000], loss: 0.12772 acc: 1.00000 val_loss: 0.19348, val_acc: 0.96667\n",
            "Epoch [4990/10000], loss: 0.12755 acc: 1.00000 val_loss: 0.19333, val_acc: 0.96667\n",
            "Epoch [5000/10000], loss: 0.12737 acc: 1.00000 val_loss: 0.19318, val_acc: 0.96667\n",
            "Epoch [5010/10000], loss: 0.12720 acc: 1.00000 val_loss: 0.19302, val_acc: 0.96667\n",
            "Epoch [5020/10000], loss: 0.12703 acc: 1.00000 val_loss: 0.19287, val_acc: 0.96667\n",
            "Epoch [5030/10000], loss: 0.12685 acc: 1.00000 val_loss: 0.19273, val_acc: 0.96667\n",
            "Epoch [5040/10000], loss: 0.12668 acc: 1.00000 val_loss: 0.19258, val_acc: 0.96667\n",
            "Epoch [5050/10000], loss: 0.12651 acc: 1.00000 val_loss: 0.19243, val_acc: 0.96667\n",
            "Epoch [5060/10000], loss: 0.12634 acc: 1.00000 val_loss: 0.19228, val_acc: 0.96667\n",
            "Epoch [5070/10000], loss: 0.12617 acc: 1.00000 val_loss: 0.19213, val_acc: 0.96667\n",
            "Epoch [5080/10000], loss: 0.12600 acc: 1.00000 val_loss: 0.19199, val_acc: 0.96667\n",
            "Epoch [5090/10000], loss: 0.12583 acc: 1.00000 val_loss: 0.19184, val_acc: 0.96667\n",
            "Epoch [5100/10000], loss: 0.12566 acc: 1.00000 val_loss: 0.19169, val_acc: 0.96667\n",
            "Epoch [5110/10000], loss: 0.12549 acc: 1.00000 val_loss: 0.19155, val_acc: 0.96667\n",
            "Epoch [5120/10000], loss: 0.12532 acc: 1.00000 val_loss: 0.19140, val_acc: 0.96667\n",
            "Epoch [5130/10000], loss: 0.12515 acc: 1.00000 val_loss: 0.19126, val_acc: 0.96667\n",
            "Epoch [5140/10000], loss: 0.12499 acc: 1.00000 val_loss: 0.19112, val_acc: 0.96667\n",
            "Epoch [5150/10000], loss: 0.12482 acc: 1.00000 val_loss: 0.19097, val_acc: 0.96667\n",
            "Epoch [5160/10000], loss: 0.12465 acc: 1.00000 val_loss: 0.19083, val_acc: 0.96667\n",
            "Epoch [5170/10000], loss: 0.12449 acc: 1.00000 val_loss: 0.19069, val_acc: 0.96667\n",
            "Epoch [5180/10000], loss: 0.12432 acc: 1.00000 val_loss: 0.19055, val_acc: 0.96667\n",
            "Epoch [5190/10000], loss: 0.12416 acc: 1.00000 val_loss: 0.19041, val_acc: 0.96667\n",
            "Epoch [5200/10000], loss: 0.12400 acc: 1.00000 val_loss: 0.19027, val_acc: 0.96667\n",
            "Epoch [5210/10000], loss: 0.12383 acc: 1.00000 val_loss: 0.19013, val_acc: 0.96667\n",
            "Epoch [5220/10000], loss: 0.12367 acc: 1.00000 val_loss: 0.18999, val_acc: 0.96667\n",
            "Epoch [5230/10000], loss: 0.12351 acc: 1.00000 val_loss: 0.18985, val_acc: 0.96667\n",
            "Epoch [5240/10000], loss: 0.12335 acc: 1.00000 val_loss: 0.18971, val_acc: 0.96667\n",
            "Epoch [5250/10000], loss: 0.12319 acc: 1.00000 val_loss: 0.18957, val_acc: 0.96667\n",
            "Epoch [5260/10000], loss: 0.12303 acc: 1.00000 val_loss: 0.18943, val_acc: 0.96667\n",
            "Epoch [5270/10000], loss: 0.12287 acc: 1.00000 val_loss: 0.18930, val_acc: 0.96667\n",
            "Epoch [5280/10000], loss: 0.12271 acc: 1.00000 val_loss: 0.18916, val_acc: 0.96667\n",
            "Epoch [5290/10000], loss: 0.12255 acc: 1.00000 val_loss: 0.18902, val_acc: 0.96667\n",
            "Epoch [5300/10000], loss: 0.12239 acc: 1.00000 val_loss: 0.18889, val_acc: 0.96667\n",
            "Epoch [5310/10000], loss: 0.12223 acc: 1.00000 val_loss: 0.18875, val_acc: 0.96667\n",
            "Epoch [5320/10000], loss: 0.12207 acc: 1.00000 val_loss: 0.18862, val_acc: 0.96667\n",
            "Epoch [5330/10000], loss: 0.12192 acc: 1.00000 val_loss: 0.18848, val_acc: 0.96667\n",
            "Epoch [5340/10000], loss: 0.12176 acc: 1.00000 val_loss: 0.18835, val_acc: 0.96667\n",
            "Epoch [5350/10000], loss: 0.12160 acc: 1.00000 val_loss: 0.18821, val_acc: 0.96667\n",
            "Epoch [5360/10000], loss: 0.12145 acc: 1.00000 val_loss: 0.18808, val_acc: 0.96667\n",
            "Epoch [5370/10000], loss: 0.12129 acc: 1.00000 val_loss: 0.18795, val_acc: 0.96667\n",
            "Epoch [5380/10000], loss: 0.12114 acc: 1.00000 val_loss: 0.18782, val_acc: 0.96667\n",
            "Epoch [5390/10000], loss: 0.12099 acc: 1.00000 val_loss: 0.18768, val_acc: 0.96667\n",
            "Epoch [5400/10000], loss: 0.12083 acc: 1.00000 val_loss: 0.18755, val_acc: 0.96667\n",
            "Epoch [5410/10000], loss: 0.12068 acc: 1.00000 val_loss: 0.18742, val_acc: 0.96667\n",
            "Epoch [5420/10000], loss: 0.12053 acc: 1.00000 val_loss: 0.18729, val_acc: 0.96667\n",
            "Epoch [5430/10000], loss: 0.12037 acc: 1.00000 val_loss: 0.18716, val_acc: 0.96667\n",
            "Epoch [5440/10000], loss: 0.12022 acc: 1.00000 val_loss: 0.18703, val_acc: 0.96667\n",
            "Epoch [5450/10000], loss: 0.12007 acc: 1.00000 val_loss: 0.18690, val_acc: 0.96667\n",
            "Epoch [5460/10000], loss: 0.11992 acc: 1.00000 val_loss: 0.18678, val_acc: 0.96667\n",
            "Epoch [5470/10000], loss: 0.11977 acc: 1.00000 val_loss: 0.18665, val_acc: 0.96667\n",
            "Epoch [5480/10000], loss: 0.11962 acc: 1.00000 val_loss: 0.18652, val_acc: 0.96667\n",
            "Epoch [5490/10000], loss: 0.11947 acc: 1.00000 val_loss: 0.18639, val_acc: 0.96667\n",
            "Epoch [5500/10000], loss: 0.11932 acc: 1.00000 val_loss: 0.18627, val_acc: 0.96667\n",
            "Epoch [5510/10000], loss: 0.11918 acc: 1.00000 val_loss: 0.18614, val_acc: 0.96667\n",
            "Epoch [5520/10000], loss: 0.11903 acc: 1.00000 val_loss: 0.18601, val_acc: 0.96667\n",
            "Epoch [5530/10000], loss: 0.11888 acc: 1.00000 val_loss: 0.18589, val_acc: 0.96667\n",
            "Epoch [5540/10000], loss: 0.11873 acc: 1.00000 val_loss: 0.18576, val_acc: 0.96667\n",
            "Epoch [5550/10000], loss: 0.11859 acc: 1.00000 val_loss: 0.18564, val_acc: 0.96667\n",
            "Epoch [5560/10000], loss: 0.11844 acc: 1.00000 val_loss: 0.18551, val_acc: 0.96667\n",
            "Epoch [5570/10000], loss: 0.11830 acc: 1.00000 val_loss: 0.18539, val_acc: 0.96667\n",
            "Epoch [5580/10000], loss: 0.11815 acc: 1.00000 val_loss: 0.18527, val_acc: 0.96667\n",
            "Epoch [5590/10000], loss: 0.11801 acc: 1.00000 val_loss: 0.18514, val_acc: 0.96667\n",
            "Epoch [5600/10000], loss: 0.11786 acc: 1.00000 val_loss: 0.18502, val_acc: 0.96667\n",
            "Epoch [5610/10000], loss: 0.11772 acc: 1.00000 val_loss: 0.18490, val_acc: 0.96667\n",
            "Epoch [5620/10000], loss: 0.11757 acc: 1.00000 val_loss: 0.18478, val_acc: 0.96667\n",
            "Epoch [5630/10000], loss: 0.11743 acc: 1.00000 val_loss: 0.18465, val_acc: 0.96667\n",
            "Epoch [5640/10000], loss: 0.11729 acc: 1.00000 val_loss: 0.18453, val_acc: 0.96667\n",
            "Epoch [5650/10000], loss: 0.11715 acc: 1.00000 val_loss: 0.18441, val_acc: 0.96667\n",
            "Epoch [5660/10000], loss: 0.11701 acc: 1.00000 val_loss: 0.18429, val_acc: 0.96667\n",
            "Epoch [5670/10000], loss: 0.11686 acc: 1.00000 val_loss: 0.18417, val_acc: 0.96667\n",
            "Epoch [5680/10000], loss: 0.11672 acc: 1.00000 val_loss: 0.18405, val_acc: 0.96667\n",
            "Epoch [5690/10000], loss: 0.11658 acc: 1.00000 val_loss: 0.18393, val_acc: 0.96667\n",
            "Epoch [5700/10000], loss: 0.11644 acc: 1.00000 val_loss: 0.18381, val_acc: 0.96667\n",
            "Epoch [5710/10000], loss: 0.11630 acc: 1.00000 val_loss: 0.18370, val_acc: 0.96667\n",
            "Epoch [5720/10000], loss: 0.11616 acc: 1.00000 val_loss: 0.18358, val_acc: 0.96667\n",
            "Epoch [5730/10000], loss: 0.11603 acc: 1.00000 val_loss: 0.18346, val_acc: 0.96667\n",
            "Epoch [5740/10000], loss: 0.11589 acc: 1.00000 val_loss: 0.18334, val_acc: 0.96667\n",
            "Epoch [5750/10000], loss: 0.11575 acc: 1.00000 val_loss: 0.18323, val_acc: 0.96667\n",
            "Epoch [5760/10000], loss: 0.11561 acc: 1.00000 val_loss: 0.18311, val_acc: 0.96667\n",
            "Epoch [5770/10000], loss: 0.11547 acc: 1.00000 val_loss: 0.18299, val_acc: 0.96667\n",
            "Epoch [5780/10000], loss: 0.11534 acc: 1.00000 val_loss: 0.18288, val_acc: 0.96667\n",
            "Epoch [5790/10000], loss: 0.11520 acc: 1.00000 val_loss: 0.18276, val_acc: 0.96667\n",
            "Epoch [5800/10000], loss: 0.11507 acc: 1.00000 val_loss: 0.18265, val_acc: 0.96667\n",
            "Epoch [5810/10000], loss: 0.11493 acc: 1.00000 val_loss: 0.18253, val_acc: 0.96667\n",
            "Epoch [5820/10000], loss: 0.11480 acc: 1.00000 val_loss: 0.18242, val_acc: 0.96667\n",
            "Epoch [5830/10000], loss: 0.11466 acc: 1.00000 val_loss: 0.18231, val_acc: 0.96667\n",
            "Epoch [5840/10000], loss: 0.11453 acc: 1.00000 val_loss: 0.18219, val_acc: 0.96667\n",
            "Epoch [5850/10000], loss: 0.11439 acc: 1.00000 val_loss: 0.18208, val_acc: 0.96667\n",
            "Epoch [5860/10000], loss: 0.11426 acc: 1.00000 val_loss: 0.18197, val_acc: 0.96667\n",
            "Epoch [5870/10000], loss: 0.11413 acc: 1.00000 val_loss: 0.18185, val_acc: 0.96667\n",
            "Epoch [5880/10000], loss: 0.11399 acc: 1.00000 val_loss: 0.18174, val_acc: 0.96667\n",
            "Epoch [5890/10000], loss: 0.11386 acc: 1.00000 val_loss: 0.18163, val_acc: 0.96667\n",
            "Epoch [5900/10000], loss: 0.11373 acc: 1.00000 val_loss: 0.18152, val_acc: 0.96667\n",
            "Epoch [5910/10000], loss: 0.11360 acc: 1.00000 val_loss: 0.18141, val_acc: 0.96667\n",
            "Epoch [5920/10000], loss: 0.11347 acc: 1.00000 val_loss: 0.18130, val_acc: 0.96667\n",
            "Epoch [5930/10000], loss: 0.11333 acc: 1.00000 val_loss: 0.18119, val_acc: 0.96667\n",
            "Epoch [5940/10000], loss: 0.11320 acc: 1.00000 val_loss: 0.18108, val_acc: 0.96667\n",
            "Epoch [5950/10000], loss: 0.11307 acc: 1.00000 val_loss: 0.18097, val_acc: 0.96667\n",
            "Epoch [5960/10000], loss: 0.11294 acc: 1.00000 val_loss: 0.18086, val_acc: 0.96667\n",
            "Epoch [5970/10000], loss: 0.11282 acc: 1.00000 val_loss: 0.18075, val_acc: 0.96667\n",
            "Epoch [5980/10000], loss: 0.11269 acc: 1.00000 val_loss: 0.18064, val_acc: 0.96667\n",
            "Epoch [5990/10000], loss: 0.11256 acc: 1.00000 val_loss: 0.18053, val_acc: 0.96667\n",
            "Epoch [6000/10000], loss: 0.11243 acc: 1.00000 val_loss: 0.18042, val_acc: 0.96667\n",
            "Epoch [6010/10000], loss: 0.11230 acc: 1.00000 val_loss: 0.18031, val_acc: 0.96667\n",
            "Epoch [6020/10000], loss: 0.11217 acc: 1.00000 val_loss: 0.18021, val_acc: 0.96667\n",
            "Epoch [6030/10000], loss: 0.11205 acc: 1.00000 val_loss: 0.18010, val_acc: 0.96667\n",
            "Epoch [6040/10000], loss: 0.11192 acc: 1.00000 val_loss: 0.17999, val_acc: 0.96667\n",
            "Epoch [6050/10000], loss: 0.11179 acc: 1.00000 val_loss: 0.17989, val_acc: 0.96667\n",
            "Epoch [6060/10000], loss: 0.11167 acc: 1.00000 val_loss: 0.17978, val_acc: 0.96667\n",
            "Epoch [6070/10000], loss: 0.11154 acc: 1.00000 val_loss: 0.17968, val_acc: 0.96667\n",
            "Epoch [6080/10000], loss: 0.11142 acc: 1.00000 val_loss: 0.17957, val_acc: 0.96667\n",
            "Epoch [6090/10000], loss: 0.11129 acc: 1.00000 val_loss: 0.17947, val_acc: 0.96667\n",
            "Epoch [6100/10000], loss: 0.11117 acc: 1.00000 val_loss: 0.17936, val_acc: 0.96667\n",
            "Epoch [6110/10000], loss: 0.11104 acc: 1.00000 val_loss: 0.17926, val_acc: 0.96667\n",
            "Epoch [6120/10000], loss: 0.11092 acc: 1.00000 val_loss: 0.17915, val_acc: 0.96667\n",
            "Epoch [6130/10000], loss: 0.11079 acc: 1.00000 val_loss: 0.17905, val_acc: 0.96667\n",
            "Epoch [6140/10000], loss: 0.11067 acc: 1.00000 val_loss: 0.17894, val_acc: 0.96667\n",
            "Epoch [6150/10000], loss: 0.11055 acc: 1.00000 val_loss: 0.17884, val_acc: 0.96667\n",
            "Epoch [6160/10000], loss: 0.11043 acc: 1.00000 val_loss: 0.17874, val_acc: 0.96667\n",
            "Epoch [6170/10000], loss: 0.11030 acc: 1.00000 val_loss: 0.17864, val_acc: 0.96667\n",
            "Epoch [6180/10000], loss: 0.11018 acc: 1.00000 val_loss: 0.17853, val_acc: 0.96667\n",
            "Epoch [6190/10000], loss: 0.11006 acc: 1.00000 val_loss: 0.17843, val_acc: 0.96667\n",
            "Epoch [6200/10000], loss: 0.10994 acc: 1.00000 val_loss: 0.17833, val_acc: 0.96667\n",
            "Epoch [6210/10000], loss: 0.10982 acc: 1.00000 val_loss: 0.17823, val_acc: 0.96667\n",
            "Epoch [6220/10000], loss: 0.10970 acc: 1.00000 val_loss: 0.17813, val_acc: 0.96667\n",
            "Epoch [6230/10000], loss: 0.10958 acc: 1.00000 val_loss: 0.17803, val_acc: 0.96667\n",
            "Epoch [6240/10000], loss: 0.10946 acc: 1.00000 val_loss: 0.17793, val_acc: 0.96667\n",
            "Epoch [6250/10000], loss: 0.10934 acc: 1.00000 val_loss: 0.17783, val_acc: 0.96667\n",
            "Epoch [6260/10000], loss: 0.10922 acc: 1.00000 val_loss: 0.17773, val_acc: 0.96667\n",
            "Epoch [6270/10000], loss: 0.10910 acc: 1.00000 val_loss: 0.17763, val_acc: 0.96667\n",
            "Epoch [6280/10000], loss: 0.10898 acc: 1.00000 val_loss: 0.17753, val_acc: 0.96667\n",
            "Epoch [6290/10000], loss: 0.10886 acc: 1.00000 val_loss: 0.17743, val_acc: 0.96667\n",
            "Epoch [6300/10000], loss: 0.10874 acc: 1.00000 val_loss: 0.17733, val_acc: 0.96667\n",
            "Epoch [6310/10000], loss: 0.10863 acc: 1.00000 val_loss: 0.17723, val_acc: 0.96667\n",
            "Epoch [6320/10000], loss: 0.10851 acc: 1.00000 val_loss: 0.17713, val_acc: 0.96667\n",
            "Epoch [6330/10000], loss: 0.10839 acc: 1.00000 val_loss: 0.17704, val_acc: 0.96667\n",
            "Epoch [6340/10000], loss: 0.10828 acc: 1.00000 val_loss: 0.17694, val_acc: 0.96667\n",
            "Epoch [6350/10000], loss: 0.10816 acc: 1.00000 val_loss: 0.17684, val_acc: 0.96667\n",
            "Epoch [6360/10000], loss: 0.10804 acc: 1.00000 val_loss: 0.17675, val_acc: 0.96667\n",
            "Epoch [6370/10000], loss: 0.10793 acc: 1.00000 val_loss: 0.17665, val_acc: 0.96667\n",
            "Epoch [6380/10000], loss: 0.10781 acc: 1.00000 val_loss: 0.17655, val_acc: 0.96667\n",
            "Epoch [6390/10000], loss: 0.10770 acc: 1.00000 val_loss: 0.17646, val_acc: 0.96667\n",
            "Epoch [6400/10000], loss: 0.10758 acc: 1.00000 val_loss: 0.17636, val_acc: 0.96667\n",
            "Epoch [6410/10000], loss: 0.10747 acc: 1.00000 val_loss: 0.17627, val_acc: 0.96667\n",
            "Epoch [6420/10000], loss: 0.10735 acc: 1.00000 val_loss: 0.17617, val_acc: 0.96667\n",
            "Epoch [6430/10000], loss: 0.10724 acc: 1.00000 val_loss: 0.17608, val_acc: 0.96667\n",
            "Epoch [6440/10000], loss: 0.10713 acc: 1.00000 val_loss: 0.17598, val_acc: 0.96667\n",
            "Epoch [6450/10000], loss: 0.10701 acc: 1.00000 val_loss: 0.17589, val_acc: 0.96667\n",
            "Epoch [6460/10000], loss: 0.10690 acc: 1.00000 val_loss: 0.17579, val_acc: 0.96667\n",
            "Epoch [6470/10000], loss: 0.10679 acc: 1.00000 val_loss: 0.17570, val_acc: 0.96667\n",
            "Epoch [6480/10000], loss: 0.10667 acc: 1.00000 val_loss: 0.17561, val_acc: 0.96667\n",
            "Epoch [6490/10000], loss: 0.10656 acc: 1.00000 val_loss: 0.17551, val_acc: 0.96667\n",
            "Epoch [6500/10000], loss: 0.10645 acc: 1.00000 val_loss: 0.17542, val_acc: 0.96667\n",
            "Epoch [6510/10000], loss: 0.10634 acc: 1.00000 val_loss: 0.17533, val_acc: 0.96667\n",
            "Epoch [6520/10000], loss: 0.10623 acc: 1.00000 val_loss: 0.17523, val_acc: 0.96667\n",
            "Epoch [6530/10000], loss: 0.10612 acc: 1.00000 val_loss: 0.17514, val_acc: 0.96667\n",
            "Epoch [6540/10000], loss: 0.10600 acc: 1.00000 val_loss: 0.17505, val_acc: 0.96667\n",
            "Epoch [6550/10000], loss: 0.10589 acc: 1.00000 val_loss: 0.17496, val_acc: 0.96667\n",
            "Epoch [6560/10000], loss: 0.10578 acc: 1.00000 val_loss: 0.17487, val_acc: 0.96667\n",
            "Epoch [6570/10000], loss: 0.10567 acc: 1.00000 val_loss: 0.17478, val_acc: 0.96667\n",
            "Epoch [6580/10000], loss: 0.10556 acc: 1.00000 val_loss: 0.17468, val_acc: 0.96667\n",
            "Epoch [6590/10000], loss: 0.10546 acc: 1.00000 val_loss: 0.17459, val_acc: 0.96667\n",
            "Epoch [6600/10000], loss: 0.10535 acc: 1.00000 val_loss: 0.17450, val_acc: 0.96667\n",
            "Epoch [6610/10000], loss: 0.10524 acc: 1.00000 val_loss: 0.17441, val_acc: 0.96667\n",
            "Epoch [6620/10000], loss: 0.10513 acc: 1.00000 val_loss: 0.17432, val_acc: 0.96667\n",
            "Epoch [6630/10000], loss: 0.10502 acc: 1.00000 val_loss: 0.17423, val_acc: 0.96667\n",
            "Epoch [6640/10000], loss: 0.10491 acc: 1.00000 val_loss: 0.17414, val_acc: 0.96667\n",
            "Epoch [6650/10000], loss: 0.10481 acc: 1.00000 val_loss: 0.17406, val_acc: 0.96667\n",
            "Epoch [6660/10000], loss: 0.10470 acc: 1.00000 val_loss: 0.17397, val_acc: 0.96667\n",
            "Epoch [6670/10000], loss: 0.10459 acc: 1.00000 val_loss: 0.17388, val_acc: 0.96667\n",
            "Epoch [6680/10000], loss: 0.10448 acc: 1.00000 val_loss: 0.17379, val_acc: 0.96667\n",
            "Epoch [6690/10000], loss: 0.10438 acc: 1.00000 val_loss: 0.17370, val_acc: 0.96667\n",
            "Epoch [6700/10000], loss: 0.10427 acc: 1.00000 val_loss: 0.17361, val_acc: 0.96667\n",
            "Epoch [6710/10000], loss: 0.10417 acc: 1.00000 val_loss: 0.17353, val_acc: 0.96667\n",
            "Epoch [6720/10000], loss: 0.10406 acc: 1.00000 val_loss: 0.17344, val_acc: 0.96667\n",
            "Epoch [6730/10000], loss: 0.10395 acc: 1.00000 val_loss: 0.17335, val_acc: 0.96667\n",
            "Epoch [6740/10000], loss: 0.10385 acc: 1.00000 val_loss: 0.17326, val_acc: 0.96667\n",
            "Epoch [6750/10000], loss: 0.10374 acc: 1.00000 val_loss: 0.17318, val_acc: 0.96667\n",
            "Epoch [6760/10000], loss: 0.10364 acc: 1.00000 val_loss: 0.17309, val_acc: 0.96667\n",
            "Epoch [6770/10000], loss: 0.10354 acc: 1.00000 val_loss: 0.17301, val_acc: 0.96667\n",
            "Epoch [6780/10000], loss: 0.10343 acc: 1.00000 val_loss: 0.17292, val_acc: 0.96667\n",
            "Epoch [6790/10000], loss: 0.10333 acc: 1.00000 val_loss: 0.17283, val_acc: 0.96667\n",
            "Epoch [6800/10000], loss: 0.10322 acc: 1.00000 val_loss: 0.17275, val_acc: 0.96667\n",
            "Epoch [6810/10000], loss: 0.10312 acc: 1.00000 val_loss: 0.17266, val_acc: 0.96667\n",
            "Epoch [6820/10000], loss: 0.10302 acc: 1.00000 val_loss: 0.17258, val_acc: 0.96667\n",
            "Epoch [6830/10000], loss: 0.10292 acc: 1.00000 val_loss: 0.17249, val_acc: 0.96667\n",
            "Epoch [6840/10000], loss: 0.10281 acc: 1.00000 val_loss: 0.17241, val_acc: 0.96667\n",
            "Epoch [6850/10000], loss: 0.10271 acc: 1.00000 val_loss: 0.17233, val_acc: 0.96667\n",
            "Epoch [6860/10000], loss: 0.10261 acc: 1.00000 val_loss: 0.17224, val_acc: 0.96667\n",
            "Epoch [6870/10000], loss: 0.10251 acc: 1.00000 val_loss: 0.17216, val_acc: 0.96667\n",
            "Epoch [6880/10000], loss: 0.10241 acc: 1.00000 val_loss: 0.17207, val_acc: 0.96667\n",
            "Epoch [6890/10000], loss: 0.10230 acc: 1.00000 val_loss: 0.17199, val_acc: 0.96667\n",
            "Epoch [6900/10000], loss: 0.10220 acc: 1.00000 val_loss: 0.17191, val_acc: 0.96667\n",
            "Epoch [6910/10000], loss: 0.10210 acc: 1.00000 val_loss: 0.17182, val_acc: 0.96667\n",
            "Epoch [6920/10000], loss: 0.10200 acc: 1.00000 val_loss: 0.17174, val_acc: 0.96667\n",
            "Epoch [6930/10000], loss: 0.10190 acc: 1.00000 val_loss: 0.17166, val_acc: 0.96667\n",
            "Epoch [6940/10000], loss: 0.10180 acc: 1.00000 val_loss: 0.17158, val_acc: 0.96667\n",
            "Epoch [6950/10000], loss: 0.10170 acc: 1.00000 val_loss: 0.17150, val_acc: 0.96667\n",
            "Epoch [6960/10000], loss: 0.10160 acc: 1.00000 val_loss: 0.17141, val_acc: 0.96667\n",
            "Epoch [6970/10000], loss: 0.10150 acc: 1.00000 val_loss: 0.17133, val_acc: 0.96667\n",
            "Epoch [6980/10000], loss: 0.10140 acc: 1.00000 val_loss: 0.17125, val_acc: 0.96667\n",
            "Epoch [6990/10000], loss: 0.10130 acc: 1.00000 val_loss: 0.17117, val_acc: 0.96667\n",
            "Epoch [7000/10000], loss: 0.10121 acc: 1.00000 val_loss: 0.17109, val_acc: 0.96667\n",
            "Epoch [7010/10000], loss: 0.10111 acc: 1.00000 val_loss: 0.17101, val_acc: 0.96667\n",
            "Epoch [7020/10000], loss: 0.10101 acc: 1.00000 val_loss: 0.17093, val_acc: 0.96667\n",
            "Epoch [7030/10000], loss: 0.10091 acc: 1.00000 val_loss: 0.17085, val_acc: 0.96667\n",
            "Epoch [7040/10000], loss: 0.10081 acc: 1.00000 val_loss: 0.17077, val_acc: 0.96667\n",
            "Epoch [7050/10000], loss: 0.10072 acc: 1.00000 val_loss: 0.17069, val_acc: 0.96667\n",
            "Epoch [7060/10000], loss: 0.10062 acc: 1.00000 val_loss: 0.17061, val_acc: 0.96667\n",
            "Epoch [7070/10000], loss: 0.10052 acc: 1.00000 val_loss: 0.17053, val_acc: 0.96667\n",
            "Epoch [7080/10000], loss: 0.10043 acc: 1.00000 val_loss: 0.17045, val_acc: 0.96667\n",
            "Epoch [7090/10000], loss: 0.10033 acc: 1.00000 val_loss: 0.17037, val_acc: 0.96667\n",
            "Epoch [7100/10000], loss: 0.10023 acc: 1.00000 val_loss: 0.17029, val_acc: 0.96667\n",
            "Epoch [7110/10000], loss: 0.10014 acc: 1.00000 val_loss: 0.17021, val_acc: 0.96667\n",
            "Epoch [7120/10000], loss: 0.10004 acc: 1.00000 val_loss: 0.17014, val_acc: 0.96667\n",
            "Epoch [7130/10000], loss: 0.09995 acc: 1.00000 val_loss: 0.17006, val_acc: 0.96667\n",
            "Epoch [7140/10000], loss: 0.09985 acc: 1.00000 val_loss: 0.16998, val_acc: 0.96667\n",
            "Epoch [7150/10000], loss: 0.09976 acc: 1.00000 val_loss: 0.16990, val_acc: 0.96667\n",
            "Epoch [7160/10000], loss: 0.09966 acc: 1.00000 val_loss: 0.16982, val_acc: 0.96667\n",
            "Epoch [7170/10000], loss: 0.09957 acc: 1.00000 val_loss: 0.16975, val_acc: 0.96667\n",
            "Epoch [7180/10000], loss: 0.09947 acc: 1.00000 val_loss: 0.16967, val_acc: 0.96667\n",
            "Epoch [7190/10000], loss: 0.09938 acc: 1.00000 val_loss: 0.16959, val_acc: 0.96667\n",
            "Epoch [7200/10000], loss: 0.09928 acc: 1.00000 val_loss: 0.16952, val_acc: 0.96667\n",
            "Epoch [7210/10000], loss: 0.09919 acc: 1.00000 val_loss: 0.16944, val_acc: 0.96667\n",
            "Epoch [7220/10000], loss: 0.09910 acc: 1.00000 val_loss: 0.16936, val_acc: 0.96667\n",
            "Epoch [7230/10000], loss: 0.09900 acc: 1.00000 val_loss: 0.16929, val_acc: 0.96667\n",
            "Epoch [7240/10000], loss: 0.09891 acc: 1.00000 val_loss: 0.16921, val_acc: 0.96667\n",
            "Epoch [7250/10000], loss: 0.09882 acc: 1.00000 val_loss: 0.16914, val_acc: 0.96667\n",
            "Epoch [7260/10000], loss: 0.09873 acc: 1.00000 val_loss: 0.16906, val_acc: 0.96667\n",
            "Epoch [7270/10000], loss: 0.09863 acc: 1.00000 val_loss: 0.16899, val_acc: 0.96667\n",
            "Epoch [7280/10000], loss: 0.09854 acc: 1.00000 val_loss: 0.16891, val_acc: 0.96667\n",
            "Epoch [7290/10000], loss: 0.09845 acc: 1.00000 val_loss: 0.16884, val_acc: 0.96667\n",
            "Epoch [7300/10000], loss: 0.09836 acc: 1.00000 val_loss: 0.16876, val_acc: 0.96667\n",
            "Epoch [7310/10000], loss: 0.09827 acc: 1.00000 val_loss: 0.16869, val_acc: 0.96667\n",
            "Epoch [7320/10000], loss: 0.09817 acc: 1.00000 val_loss: 0.16861, val_acc: 0.96667\n",
            "Epoch [7330/10000], loss: 0.09808 acc: 1.00000 val_loss: 0.16854, val_acc: 0.96667\n",
            "Epoch [7340/10000], loss: 0.09799 acc: 1.00000 val_loss: 0.16846, val_acc: 0.96667\n",
            "Epoch [7350/10000], loss: 0.09790 acc: 1.00000 val_loss: 0.16839, val_acc: 0.96667\n",
            "Epoch [7360/10000], loss: 0.09781 acc: 1.00000 val_loss: 0.16832, val_acc: 0.96667\n",
            "Epoch [7370/10000], loss: 0.09772 acc: 1.00000 val_loss: 0.16824, val_acc: 0.96667\n",
            "Epoch [7380/10000], loss: 0.09763 acc: 1.00000 val_loss: 0.16817, val_acc: 0.96667\n",
            "Epoch [7390/10000], loss: 0.09754 acc: 1.00000 val_loss: 0.16810, val_acc: 0.96667\n",
            "Epoch [7400/10000], loss: 0.09745 acc: 1.00000 val_loss: 0.16802, val_acc: 0.96667\n",
            "Epoch [7410/10000], loss: 0.09736 acc: 1.00000 val_loss: 0.16795, val_acc: 0.96667\n",
            "Epoch [7420/10000], loss: 0.09727 acc: 1.00000 val_loss: 0.16788, val_acc: 0.96667\n",
            "Epoch [7430/10000], loss: 0.09718 acc: 1.00000 val_loss: 0.16781, val_acc: 0.96667\n",
            "Epoch [7440/10000], loss: 0.09710 acc: 1.00000 val_loss: 0.16774, val_acc: 0.96667\n",
            "Epoch [7450/10000], loss: 0.09701 acc: 1.00000 val_loss: 0.16766, val_acc: 0.96667\n",
            "Epoch [7460/10000], loss: 0.09692 acc: 1.00000 val_loss: 0.16759, val_acc: 0.96667\n",
            "Epoch [7470/10000], loss: 0.09683 acc: 1.00000 val_loss: 0.16752, val_acc: 0.96667\n",
            "Epoch [7480/10000], loss: 0.09674 acc: 1.00000 val_loss: 0.16745, val_acc: 0.96667\n",
            "Epoch [7490/10000], loss: 0.09665 acc: 1.00000 val_loss: 0.16738, val_acc: 0.96667\n",
            "Epoch [7500/10000], loss: 0.09657 acc: 1.00000 val_loss: 0.16731, val_acc: 0.96667\n",
            "Epoch [7510/10000], loss: 0.09648 acc: 1.00000 val_loss: 0.16724, val_acc: 0.96667\n",
            "Epoch [7520/10000], loss: 0.09639 acc: 1.00000 val_loss: 0.16717, val_acc: 0.96667\n",
            "Epoch [7530/10000], loss: 0.09631 acc: 1.00000 val_loss: 0.16710, val_acc: 0.96667\n",
            "Epoch [7540/10000], loss: 0.09622 acc: 1.00000 val_loss: 0.16703, val_acc: 0.96667\n",
            "Epoch [7550/10000], loss: 0.09613 acc: 1.00000 val_loss: 0.16696, val_acc: 0.96667\n",
            "Epoch [7560/10000], loss: 0.09605 acc: 1.00000 val_loss: 0.16689, val_acc: 0.96667\n",
            "Epoch [7570/10000], loss: 0.09596 acc: 1.00000 val_loss: 0.16682, val_acc: 0.96667\n",
            "Epoch [7580/10000], loss: 0.09587 acc: 1.00000 val_loss: 0.16675, val_acc: 0.96667\n",
            "Epoch [7590/10000], loss: 0.09579 acc: 1.00000 val_loss: 0.16668, val_acc: 0.96667\n",
            "Epoch [7600/10000], loss: 0.09570 acc: 1.00000 val_loss: 0.16661, val_acc: 0.96667\n",
            "Epoch [7610/10000], loss: 0.09562 acc: 1.00000 val_loss: 0.16654, val_acc: 0.96667\n",
            "Epoch [7620/10000], loss: 0.09553 acc: 1.00000 val_loss: 0.16647, val_acc: 0.96667\n",
            "Epoch [7630/10000], loss: 0.09545 acc: 1.00000 val_loss: 0.16640, val_acc: 0.96667\n",
            "Epoch [7640/10000], loss: 0.09536 acc: 1.00000 val_loss: 0.16633, val_acc: 0.96667\n",
            "Epoch [7650/10000], loss: 0.09528 acc: 1.00000 val_loss: 0.16626, val_acc: 0.96667\n",
            "Epoch [7660/10000], loss: 0.09519 acc: 1.00000 val_loss: 0.16620, val_acc: 0.96667\n",
            "Epoch [7670/10000], loss: 0.09511 acc: 1.00000 val_loss: 0.16613, val_acc: 0.96667\n",
            "Epoch [7680/10000], loss: 0.09502 acc: 1.00000 val_loss: 0.16606, val_acc: 0.96667\n",
            "Epoch [7690/10000], loss: 0.09494 acc: 1.00000 val_loss: 0.16599, val_acc: 0.96667\n",
            "Epoch [7700/10000], loss: 0.09486 acc: 1.00000 val_loss: 0.16593, val_acc: 0.96667\n",
            "Epoch [7710/10000], loss: 0.09477 acc: 1.00000 val_loss: 0.16586, val_acc: 0.96667\n",
            "Epoch [7720/10000], loss: 0.09469 acc: 1.00000 val_loss: 0.16579, val_acc: 0.96667\n",
            "Epoch [7730/10000], loss: 0.09461 acc: 1.00000 val_loss: 0.16572, val_acc: 0.96667\n",
            "Epoch [7740/10000], loss: 0.09452 acc: 1.00000 val_loss: 0.16566, val_acc: 0.96667\n",
            "Epoch [7750/10000], loss: 0.09444 acc: 1.00000 val_loss: 0.16559, val_acc: 0.96667\n",
            "Epoch [7760/10000], loss: 0.09436 acc: 1.00000 val_loss: 0.16552, val_acc: 0.96667\n",
            "Epoch [7770/10000], loss: 0.09428 acc: 1.00000 val_loss: 0.16546, val_acc: 0.96667\n",
            "Epoch [7780/10000], loss: 0.09419 acc: 1.00000 val_loss: 0.16539, val_acc: 0.96667\n",
            "Epoch [7790/10000], loss: 0.09411 acc: 1.00000 val_loss: 0.16533, val_acc: 0.96667\n",
            "Epoch [7800/10000], loss: 0.09403 acc: 1.00000 val_loss: 0.16526, val_acc: 0.96667\n",
            "Epoch [7810/10000], loss: 0.09395 acc: 1.00000 val_loss: 0.16519, val_acc: 0.96667\n",
            "Epoch [7820/10000], loss: 0.09387 acc: 1.00000 val_loss: 0.16513, val_acc: 0.96667\n",
            "Epoch [7830/10000], loss: 0.09378 acc: 1.00000 val_loss: 0.16506, val_acc: 0.96667\n",
            "Epoch [7840/10000], loss: 0.09370 acc: 1.00000 val_loss: 0.16500, val_acc: 0.96667\n",
            "Epoch [7850/10000], loss: 0.09362 acc: 1.00000 val_loss: 0.16493, val_acc: 0.96667\n",
            "Epoch [7860/10000], loss: 0.09354 acc: 1.00000 val_loss: 0.16487, val_acc: 0.96667\n",
            "Epoch [7870/10000], loss: 0.09346 acc: 1.00000 val_loss: 0.16480, val_acc: 0.96667\n",
            "Epoch [7880/10000], loss: 0.09338 acc: 1.00000 val_loss: 0.16474, val_acc: 0.96667\n",
            "Epoch [7890/10000], loss: 0.09330 acc: 1.00000 val_loss: 0.16468, val_acc: 0.96667\n",
            "Epoch [7900/10000], loss: 0.09322 acc: 1.00000 val_loss: 0.16461, val_acc: 0.96667\n",
            "Epoch [7910/10000], loss: 0.09314 acc: 1.00000 val_loss: 0.16455, val_acc: 0.96667\n",
            "Epoch [7920/10000], loss: 0.09306 acc: 1.00000 val_loss: 0.16448, val_acc: 0.96667\n",
            "Epoch [7930/10000], loss: 0.09298 acc: 1.00000 val_loss: 0.16442, val_acc: 0.96667\n",
            "Epoch [7940/10000], loss: 0.09290 acc: 1.00000 val_loss: 0.16436, val_acc: 0.96667\n",
            "Epoch [7950/10000], loss: 0.09282 acc: 1.00000 val_loss: 0.16429, val_acc: 0.96667\n",
            "Epoch [7960/10000], loss: 0.09274 acc: 1.00000 val_loss: 0.16423, val_acc: 0.96667\n",
            "Epoch [7970/10000], loss: 0.09266 acc: 1.00000 val_loss: 0.16417, val_acc: 0.96667\n",
            "Epoch [7980/10000], loss: 0.09259 acc: 1.00000 val_loss: 0.16410, val_acc: 0.96667\n",
            "Epoch [7990/10000], loss: 0.09251 acc: 1.00000 val_loss: 0.16404, val_acc: 0.96667\n",
            "Epoch [8000/10000], loss: 0.09243 acc: 1.00000 val_loss: 0.16398, val_acc: 0.96667\n",
            "Epoch [8010/10000], loss: 0.09235 acc: 1.00000 val_loss: 0.16392, val_acc: 0.96667\n",
            "Epoch [8020/10000], loss: 0.09227 acc: 1.00000 val_loss: 0.16385, val_acc: 0.96667\n",
            "Epoch [8030/10000], loss: 0.09219 acc: 1.00000 val_loss: 0.16379, val_acc: 0.96667\n",
            "Epoch [8040/10000], loss: 0.09212 acc: 1.00000 val_loss: 0.16373, val_acc: 0.96667\n",
            "Epoch [8050/10000], loss: 0.09204 acc: 1.00000 val_loss: 0.16367, val_acc: 0.96667\n",
            "Epoch [8060/10000], loss: 0.09196 acc: 1.00000 val_loss: 0.16361, val_acc: 0.96667\n",
            "Epoch [8070/10000], loss: 0.09188 acc: 1.00000 val_loss: 0.16354, val_acc: 0.96667\n",
            "Epoch [8080/10000], loss: 0.09181 acc: 1.00000 val_loss: 0.16348, val_acc: 0.96667\n",
            "Epoch [8090/10000], loss: 0.09173 acc: 1.00000 val_loss: 0.16342, val_acc: 0.96667\n",
            "Epoch [8100/10000], loss: 0.09165 acc: 1.00000 val_loss: 0.16336, val_acc: 0.96667\n",
            "Epoch [8110/10000], loss: 0.09158 acc: 1.00000 val_loss: 0.16330, val_acc: 0.96667\n",
            "Epoch [8120/10000], loss: 0.09150 acc: 1.00000 val_loss: 0.16324, val_acc: 0.96667\n",
            "Epoch [8130/10000], loss: 0.09142 acc: 1.00000 val_loss: 0.16318, val_acc: 0.96667\n",
            "Epoch [8140/10000], loss: 0.09135 acc: 1.00000 val_loss: 0.16312, val_acc: 0.96667\n",
            "Epoch [8150/10000], loss: 0.09127 acc: 1.00000 val_loss: 0.16306, val_acc: 0.96667\n",
            "Epoch [8160/10000], loss: 0.09120 acc: 1.00000 val_loss: 0.16300, val_acc: 0.96667\n",
            "Epoch [8170/10000], loss: 0.09112 acc: 1.00000 val_loss: 0.16294, val_acc: 0.96667\n",
            "Epoch [8180/10000], loss: 0.09105 acc: 1.00000 val_loss: 0.16288, val_acc: 0.96667\n",
            "Epoch [8190/10000], loss: 0.09097 acc: 1.00000 val_loss: 0.16282, val_acc: 0.96667\n",
            "Epoch [8200/10000], loss: 0.09089 acc: 1.00000 val_loss: 0.16276, val_acc: 0.96667\n",
            "Epoch [8210/10000], loss: 0.09082 acc: 1.00000 val_loss: 0.16270, val_acc: 0.96667\n",
            "Epoch [8220/10000], loss: 0.09074 acc: 1.00000 val_loss: 0.16264, val_acc: 0.96667\n",
            "Epoch [8230/10000], loss: 0.09067 acc: 1.00000 val_loss: 0.16258, val_acc: 0.96667\n",
            "Epoch [8240/10000], loss: 0.09060 acc: 1.00000 val_loss: 0.16252, val_acc: 0.96667\n",
            "Epoch [8250/10000], loss: 0.09052 acc: 1.00000 val_loss: 0.16246, val_acc: 0.96667\n",
            "Epoch [8260/10000], loss: 0.09045 acc: 1.00000 val_loss: 0.16240, val_acc: 0.96667\n",
            "Epoch [8270/10000], loss: 0.09037 acc: 1.00000 val_loss: 0.16234, val_acc: 0.96667\n",
            "Epoch [8280/10000], loss: 0.09030 acc: 1.00000 val_loss: 0.16228, val_acc: 0.96667\n",
            "Epoch [8290/10000], loss: 0.09023 acc: 1.00000 val_loss: 0.16222, val_acc: 0.96667\n",
            "Epoch [8300/10000], loss: 0.09015 acc: 1.00000 val_loss: 0.16217, val_acc: 0.96667\n",
            "Epoch [8310/10000], loss: 0.09008 acc: 1.00000 val_loss: 0.16211, val_acc: 0.96667\n",
            "Epoch [8320/10000], loss: 0.09000 acc: 1.00000 val_loss: 0.16205, val_acc: 0.96667\n",
            "Epoch [8330/10000], loss: 0.08993 acc: 1.00000 val_loss: 0.16199, val_acc: 0.96667\n",
            "Epoch [8340/10000], loss: 0.08986 acc: 1.00000 val_loss: 0.16193, val_acc: 0.96667\n",
            "Epoch [8350/10000], loss: 0.08979 acc: 1.00000 val_loss: 0.16188, val_acc: 0.96667\n",
            "Epoch [8360/10000], loss: 0.08971 acc: 1.00000 val_loss: 0.16182, val_acc: 0.96667\n",
            "Epoch [8370/10000], loss: 0.08964 acc: 1.00000 val_loss: 0.16176, val_acc: 0.96667\n",
            "Epoch [8380/10000], loss: 0.08957 acc: 1.00000 val_loss: 0.16170, val_acc: 0.96667\n",
            "Epoch [8390/10000], loss: 0.08950 acc: 1.00000 val_loss: 0.16165, val_acc: 0.96667\n",
            "Epoch [8400/10000], loss: 0.08942 acc: 1.00000 val_loss: 0.16159, val_acc: 0.96667\n",
            "Epoch [8410/10000], loss: 0.08935 acc: 1.00000 val_loss: 0.16153, val_acc: 0.96667\n",
            "Epoch [8420/10000], loss: 0.08928 acc: 1.00000 val_loss: 0.16148, val_acc: 0.96667\n",
            "Epoch [8430/10000], loss: 0.08921 acc: 1.00000 val_loss: 0.16142, val_acc: 0.96667\n",
            "Epoch [8440/10000], loss: 0.08914 acc: 1.00000 val_loss: 0.16136, val_acc: 0.96667\n",
            "Epoch [8450/10000], loss: 0.08907 acc: 1.00000 val_loss: 0.16131, val_acc: 0.96667\n",
            "Epoch [8460/10000], loss: 0.08899 acc: 1.00000 val_loss: 0.16125, val_acc: 0.96667\n",
            "Epoch [8470/10000], loss: 0.08892 acc: 1.00000 val_loss: 0.16119, val_acc: 0.96667\n",
            "Epoch [8480/10000], loss: 0.08885 acc: 1.00000 val_loss: 0.16114, val_acc: 0.96667\n",
            "Epoch [8490/10000], loss: 0.08878 acc: 1.00000 val_loss: 0.16108, val_acc: 0.96667\n",
            "Epoch [8500/10000], loss: 0.08871 acc: 1.00000 val_loss: 0.16103, val_acc: 0.96667\n",
            "Epoch [8510/10000], loss: 0.08864 acc: 1.00000 val_loss: 0.16097, val_acc: 0.96667\n",
            "Epoch [8520/10000], loss: 0.08857 acc: 1.00000 val_loss: 0.16092, val_acc: 0.96667\n",
            "Epoch [8530/10000], loss: 0.08850 acc: 1.00000 val_loss: 0.16086, val_acc: 0.96667\n",
            "Epoch [8540/10000], loss: 0.08843 acc: 1.00000 val_loss: 0.16081, val_acc: 0.96667\n",
            "Epoch [8550/10000], loss: 0.08836 acc: 1.00000 val_loss: 0.16075, val_acc: 0.96667\n",
            "Epoch [8560/10000], loss: 0.08829 acc: 1.00000 val_loss: 0.16070, val_acc: 0.96667\n",
            "Epoch [8570/10000], loss: 0.08822 acc: 1.00000 val_loss: 0.16064, val_acc: 0.96667\n",
            "Epoch [8580/10000], loss: 0.08815 acc: 1.00000 val_loss: 0.16059, val_acc: 0.96667\n",
            "Epoch [8590/10000], loss: 0.08808 acc: 1.00000 val_loss: 0.16053, val_acc: 0.96667\n",
            "Epoch [8600/10000], loss: 0.08801 acc: 1.00000 val_loss: 0.16048, val_acc: 0.96667\n",
            "Epoch [8610/10000], loss: 0.08794 acc: 1.00000 val_loss: 0.16042, val_acc: 0.96667\n",
            "Epoch [8620/10000], loss: 0.08787 acc: 1.00000 val_loss: 0.16037, val_acc: 0.96667\n",
            "Epoch [8630/10000], loss: 0.08780 acc: 1.00000 val_loss: 0.16031, val_acc: 0.96667\n",
            "Epoch [8640/10000], loss: 0.08774 acc: 1.00000 val_loss: 0.16026, val_acc: 0.96667\n",
            "Epoch [8650/10000], loss: 0.08767 acc: 1.00000 val_loss: 0.16021, val_acc: 0.96667\n",
            "Epoch [8660/10000], loss: 0.08760 acc: 1.00000 val_loss: 0.16015, val_acc: 0.96667\n",
            "Epoch [8670/10000], loss: 0.08753 acc: 1.00000 val_loss: 0.16010, val_acc: 0.96667\n",
            "Epoch [8680/10000], loss: 0.08746 acc: 1.00000 val_loss: 0.16005, val_acc: 0.96667\n",
            "Epoch [8690/10000], loss: 0.08739 acc: 1.00000 val_loss: 0.15999, val_acc: 0.96667\n",
            "Epoch [8700/10000], loss: 0.08733 acc: 1.00000 val_loss: 0.15994, val_acc: 0.96667\n",
            "Epoch [8710/10000], loss: 0.08726 acc: 1.00000 val_loss: 0.15989, val_acc: 0.96667\n",
            "Epoch [8720/10000], loss: 0.08719 acc: 1.00000 val_loss: 0.15983, val_acc: 0.96667\n",
            "Epoch [8730/10000], loss: 0.08712 acc: 1.00000 val_loss: 0.15978, val_acc: 0.96667\n",
            "Epoch [8740/10000], loss: 0.08706 acc: 1.00000 val_loss: 0.15973, val_acc: 0.96667\n",
            "Epoch [8750/10000], loss: 0.08699 acc: 1.00000 val_loss: 0.15967, val_acc: 0.96667\n",
            "Epoch [8760/10000], loss: 0.08692 acc: 1.00000 val_loss: 0.15962, val_acc: 0.96667\n",
            "Epoch [8770/10000], loss: 0.08685 acc: 1.00000 val_loss: 0.15957, val_acc: 0.96667\n",
            "Epoch [8780/10000], loss: 0.08679 acc: 1.00000 val_loss: 0.15952, val_acc: 0.96667\n",
            "Epoch [8790/10000], loss: 0.08672 acc: 1.00000 val_loss: 0.15946, val_acc: 0.96667\n",
            "Epoch [8800/10000], loss: 0.08665 acc: 1.00000 val_loss: 0.15941, val_acc: 0.96667\n",
            "Epoch [8810/10000], loss: 0.08659 acc: 1.00000 val_loss: 0.15936, val_acc: 0.96667\n",
            "Epoch [8820/10000], loss: 0.08652 acc: 1.00000 val_loss: 0.15931, val_acc: 0.96667\n",
            "Epoch [8830/10000], loss: 0.08646 acc: 1.00000 val_loss: 0.15926, val_acc: 0.96667\n",
            "Epoch [8840/10000], loss: 0.08639 acc: 1.00000 val_loss: 0.15921, val_acc: 0.96667\n",
            "Epoch [8850/10000], loss: 0.08632 acc: 1.00000 val_loss: 0.15915, val_acc: 0.96667\n",
            "Epoch [8860/10000], loss: 0.08626 acc: 1.00000 val_loss: 0.15910, val_acc: 0.96667\n",
            "Epoch [8870/10000], loss: 0.08619 acc: 1.00000 val_loss: 0.15905, val_acc: 0.96667\n",
            "Epoch [8880/10000], loss: 0.08613 acc: 1.00000 val_loss: 0.15900, val_acc: 0.96667\n",
            "Epoch [8890/10000], loss: 0.08606 acc: 1.00000 val_loss: 0.15895, val_acc: 0.96667\n",
            "Epoch [8900/10000], loss: 0.08600 acc: 1.00000 val_loss: 0.15890, val_acc: 0.96667\n",
            "Epoch [8910/10000], loss: 0.08593 acc: 1.00000 val_loss: 0.15885, val_acc: 0.96667\n",
            "Epoch [8920/10000], loss: 0.08587 acc: 1.00000 val_loss: 0.15880, val_acc: 0.96667\n",
            "Epoch [8930/10000], loss: 0.08580 acc: 1.00000 val_loss: 0.15875, val_acc: 0.96667\n",
            "Epoch [8940/10000], loss: 0.08574 acc: 1.00000 val_loss: 0.15870, val_acc: 0.96667\n",
            "Epoch [8950/10000], loss: 0.08567 acc: 1.00000 val_loss: 0.15865, val_acc: 0.96667\n",
            "Epoch [8960/10000], loss: 0.08561 acc: 1.00000 val_loss: 0.15859, val_acc: 0.96667\n",
            "Epoch [8970/10000], loss: 0.08554 acc: 1.00000 val_loss: 0.15854, val_acc: 0.96667\n",
            "Epoch [8980/10000], loss: 0.08548 acc: 1.00000 val_loss: 0.15849, val_acc: 0.96667\n",
            "Epoch [8990/10000], loss: 0.08541 acc: 1.00000 val_loss: 0.15844, val_acc: 0.96667\n",
            "Epoch [9000/10000], loss: 0.08535 acc: 1.00000 val_loss: 0.15839, val_acc: 0.96667\n",
            "Epoch [9010/10000], loss: 0.08529 acc: 1.00000 val_loss: 0.15834, val_acc: 0.96667\n",
            "Epoch [9020/10000], loss: 0.08522 acc: 1.00000 val_loss: 0.15830, val_acc: 0.96667\n",
            "Epoch [9030/10000], loss: 0.08516 acc: 1.00000 val_loss: 0.15825, val_acc: 0.96667\n",
            "Epoch [9040/10000], loss: 0.08509 acc: 1.00000 val_loss: 0.15820, val_acc: 0.96667\n",
            "Epoch [9050/10000], loss: 0.08503 acc: 1.00000 val_loss: 0.15815, val_acc: 0.96667\n",
            "Epoch [9060/10000], loss: 0.08497 acc: 1.00000 val_loss: 0.15810, val_acc: 0.96667\n",
            "Epoch [9070/10000], loss: 0.08490 acc: 1.00000 val_loss: 0.15805, val_acc: 0.96667\n",
            "Epoch [9080/10000], loss: 0.08484 acc: 1.00000 val_loss: 0.15800, val_acc: 0.96667\n",
            "Epoch [9090/10000], loss: 0.08478 acc: 1.00000 val_loss: 0.15795, val_acc: 0.96667\n",
            "Epoch [9100/10000], loss: 0.08472 acc: 1.00000 val_loss: 0.15790, val_acc: 0.96667\n",
            "Epoch [9110/10000], loss: 0.08465 acc: 1.00000 val_loss: 0.15785, val_acc: 0.96667\n",
            "Epoch [9120/10000], loss: 0.08459 acc: 1.00000 val_loss: 0.15781, val_acc: 0.96667\n",
            "Epoch [9130/10000], loss: 0.08453 acc: 1.00000 val_loss: 0.15776, val_acc: 0.96667\n",
            "Epoch [9140/10000], loss: 0.08446 acc: 1.00000 val_loss: 0.15771, val_acc: 0.96667\n",
            "Epoch [9150/10000], loss: 0.08440 acc: 1.00000 val_loss: 0.15766, val_acc: 0.96667\n",
            "Epoch [9160/10000], loss: 0.08434 acc: 1.00000 val_loss: 0.15761, val_acc: 0.96667\n",
            "Epoch [9170/10000], loss: 0.08428 acc: 1.00000 val_loss: 0.15756, val_acc: 0.96667\n",
            "Epoch [9180/10000], loss: 0.08422 acc: 1.00000 val_loss: 0.15752, val_acc: 0.96667\n",
            "Epoch [9190/10000], loss: 0.08415 acc: 1.00000 val_loss: 0.15747, val_acc: 0.96667\n",
            "Epoch [9200/10000], loss: 0.08409 acc: 1.00000 val_loss: 0.15742, val_acc: 0.96667\n",
            "Epoch [9210/10000], loss: 0.08403 acc: 1.00000 val_loss: 0.15737, val_acc: 0.96667\n",
            "Epoch [9220/10000], loss: 0.08397 acc: 1.00000 val_loss: 0.15733, val_acc: 0.96667\n",
            "Epoch [9230/10000], loss: 0.08391 acc: 1.00000 val_loss: 0.15728, val_acc: 0.96667\n",
            "Epoch [9240/10000], loss: 0.08385 acc: 1.00000 val_loss: 0.15723, val_acc: 0.96667\n",
            "Epoch [9250/10000], loss: 0.08379 acc: 1.00000 val_loss: 0.15718, val_acc: 0.96667\n",
            "Epoch [9260/10000], loss: 0.08372 acc: 1.00000 val_loss: 0.15714, val_acc: 0.96667\n",
            "Epoch [9270/10000], loss: 0.08366 acc: 1.00000 val_loss: 0.15709, val_acc: 0.96667\n",
            "Epoch [9280/10000], loss: 0.08360 acc: 1.00000 val_loss: 0.15704, val_acc: 0.96667\n",
            "Epoch [9290/10000], loss: 0.08354 acc: 1.00000 val_loss: 0.15700, val_acc: 0.96667\n",
            "Epoch [9300/10000], loss: 0.08348 acc: 1.00000 val_loss: 0.15695, val_acc: 0.96667\n",
            "Epoch [9310/10000], loss: 0.08342 acc: 1.00000 val_loss: 0.15690, val_acc: 0.96667\n",
            "Epoch [9320/10000], loss: 0.08336 acc: 1.00000 val_loss: 0.15686, val_acc: 0.96667\n",
            "Epoch [9330/10000], loss: 0.08330 acc: 1.00000 val_loss: 0.15681, val_acc: 0.96667\n",
            "Epoch [9340/10000], loss: 0.08324 acc: 1.00000 val_loss: 0.15676, val_acc: 0.96667\n",
            "Epoch [9350/10000], loss: 0.08318 acc: 1.00000 val_loss: 0.15672, val_acc: 0.96667\n",
            "Epoch [9360/10000], loss: 0.08312 acc: 1.00000 val_loss: 0.15667, val_acc: 0.96667\n",
            "Epoch [9370/10000], loss: 0.08306 acc: 1.00000 val_loss: 0.15662, val_acc: 0.96667\n",
            "Epoch [9380/10000], loss: 0.08300 acc: 1.00000 val_loss: 0.15658, val_acc: 0.96667\n",
            "Epoch [9390/10000], loss: 0.08294 acc: 1.00000 val_loss: 0.15653, val_acc: 0.96667\n",
            "Epoch [9400/10000], loss: 0.08288 acc: 1.00000 val_loss: 0.15649, val_acc: 0.96667\n",
            "Epoch [9410/10000], loss: 0.08282 acc: 1.00000 val_loss: 0.15644, val_acc: 0.96667\n",
            "Epoch [9420/10000], loss: 0.08276 acc: 1.00000 val_loss: 0.15640, val_acc: 0.96667\n",
            "Epoch [9430/10000], loss: 0.08270 acc: 1.00000 val_loss: 0.15635, val_acc: 0.96667\n",
            "Epoch [9440/10000], loss: 0.08265 acc: 1.00000 val_loss: 0.15630, val_acc: 0.96667\n",
            "Epoch [9450/10000], loss: 0.08259 acc: 1.00000 val_loss: 0.15626, val_acc: 0.96667\n",
            "Epoch [9460/10000], loss: 0.08253 acc: 1.00000 val_loss: 0.15621, val_acc: 0.96667\n",
            "Epoch [9470/10000], loss: 0.08247 acc: 1.00000 val_loss: 0.15617, val_acc: 0.96667\n",
            "Epoch [9480/10000], loss: 0.08241 acc: 1.00000 val_loss: 0.15612, val_acc: 0.96667\n",
            "Epoch [9490/10000], loss: 0.08235 acc: 1.00000 val_loss: 0.15608, val_acc: 0.96667\n",
            "Epoch [9500/10000], loss: 0.08229 acc: 1.00000 val_loss: 0.15603, val_acc: 0.96667\n",
            "Epoch [9510/10000], loss: 0.08224 acc: 1.00000 val_loss: 0.15599, val_acc: 0.96667\n",
            "Epoch [9520/10000], loss: 0.08218 acc: 1.00000 val_loss: 0.15594, val_acc: 0.96667\n",
            "Epoch [9530/10000], loss: 0.08212 acc: 1.00000 val_loss: 0.15590, val_acc: 0.96667\n",
            "Epoch [9540/10000], loss: 0.08206 acc: 1.00000 val_loss: 0.15586, val_acc: 0.96667\n",
            "Epoch [9550/10000], loss: 0.08200 acc: 1.00000 val_loss: 0.15581, val_acc: 0.96667\n",
            "Epoch [9560/10000], loss: 0.08195 acc: 1.00000 val_loss: 0.15577, val_acc: 0.96667\n",
            "Epoch [9570/10000], loss: 0.08189 acc: 1.00000 val_loss: 0.15572, val_acc: 0.96667\n",
            "Epoch [9580/10000], loss: 0.08183 acc: 1.00000 val_loss: 0.15568, val_acc: 0.96667\n",
            "Epoch [9590/10000], loss: 0.08177 acc: 1.00000 val_loss: 0.15563, val_acc: 0.96667\n",
            "Epoch [9600/10000], loss: 0.08172 acc: 1.00000 val_loss: 0.15559, val_acc: 0.96667\n",
            "Epoch [9610/10000], loss: 0.08166 acc: 1.00000 val_loss: 0.15555, val_acc: 0.96667\n",
            "Epoch [9620/10000], loss: 0.08160 acc: 1.00000 val_loss: 0.15550, val_acc: 0.96667\n",
            "Epoch [9630/10000], loss: 0.08154 acc: 1.00000 val_loss: 0.15546, val_acc: 0.96667\n",
            "Epoch [9640/10000], loss: 0.08149 acc: 1.00000 val_loss: 0.15542, val_acc: 0.96667\n",
            "Epoch [9650/10000], loss: 0.08143 acc: 1.00000 val_loss: 0.15537, val_acc: 0.96667\n",
            "Epoch [9660/10000], loss: 0.08137 acc: 1.00000 val_loss: 0.15533, val_acc: 0.96667\n",
            "Epoch [9670/10000], loss: 0.08132 acc: 1.00000 val_loss: 0.15529, val_acc: 0.96667\n",
            "Epoch [9680/10000], loss: 0.08126 acc: 1.00000 val_loss: 0.15524, val_acc: 0.96667\n",
            "Epoch [9690/10000], loss: 0.08120 acc: 1.00000 val_loss: 0.15520, val_acc: 0.96667\n",
            "Epoch [9700/10000], loss: 0.08115 acc: 1.00000 val_loss: 0.15516, val_acc: 0.96667\n",
            "Epoch [9710/10000], loss: 0.08109 acc: 1.00000 val_loss: 0.15511, val_acc: 0.96667\n",
            "Epoch [9720/10000], loss: 0.08103 acc: 1.00000 val_loss: 0.15507, val_acc: 0.96667\n",
            "Epoch [9730/10000], loss: 0.08098 acc: 1.00000 val_loss: 0.15503, val_acc: 0.96667\n",
            "Epoch [9740/10000], loss: 0.08092 acc: 1.00000 val_loss: 0.15499, val_acc: 0.96667\n",
            "Epoch [9750/10000], loss: 0.08087 acc: 1.00000 val_loss: 0.15494, val_acc: 0.96667\n",
            "Epoch [9760/10000], loss: 0.08081 acc: 1.00000 val_loss: 0.15490, val_acc: 0.96667\n",
            "Epoch [9770/10000], loss: 0.08076 acc: 1.00000 val_loss: 0.15486, val_acc: 0.96667\n",
            "Epoch [9780/10000], loss: 0.08070 acc: 1.00000 val_loss: 0.15482, val_acc: 0.96667\n",
            "Epoch [9790/10000], loss: 0.08064 acc: 1.00000 val_loss: 0.15477, val_acc: 0.96667\n",
            "Epoch [9800/10000], loss: 0.08059 acc: 1.00000 val_loss: 0.15473, val_acc: 0.96667\n",
            "Epoch [9810/10000], loss: 0.08053 acc: 1.00000 val_loss: 0.15469, val_acc: 0.96667\n",
            "Epoch [9820/10000], loss: 0.08048 acc: 1.00000 val_loss: 0.15465, val_acc: 0.96667\n",
            "Epoch [9830/10000], loss: 0.08042 acc: 1.00000 val_loss: 0.15461, val_acc: 0.96667\n",
            "Epoch [9840/10000], loss: 0.08037 acc: 1.00000 val_loss: 0.15456, val_acc: 0.96667\n",
            "Epoch [9850/10000], loss: 0.08031 acc: 1.00000 val_loss: 0.15452, val_acc: 0.96667\n",
            "Epoch [9860/10000], loss: 0.08026 acc: 1.00000 val_loss: 0.15448, val_acc: 0.96667\n",
            "Epoch [9870/10000], loss: 0.08020 acc: 1.00000 val_loss: 0.15444, val_acc: 0.96667\n",
            "Epoch [9880/10000], loss: 0.08015 acc: 1.00000 val_loss: 0.15440, val_acc: 0.96667\n",
            "Epoch [9890/10000], loss: 0.08009 acc: 1.00000 val_loss: 0.15436, val_acc: 0.96667\n",
            "Epoch [9900/10000], loss: 0.08004 acc: 1.00000 val_loss: 0.15432, val_acc: 0.96667\n",
            "Epoch [9910/10000], loss: 0.07999 acc: 1.00000 val_loss: 0.15427, val_acc: 0.96667\n",
            "Epoch [9920/10000], loss: 0.07993 acc: 1.00000 val_loss: 0.15423, val_acc: 0.96667\n",
            "Epoch [9930/10000], loss: 0.07988 acc: 1.00000 val_loss: 0.15419, val_acc: 0.96667\n",
            "Epoch [9940/10000], loss: 0.07982 acc: 1.00000 val_loss: 0.15415, val_acc: 0.96667\n",
            "Epoch [9950/10000], loss: 0.07977 acc: 1.00000 val_loss: 0.15411, val_acc: 0.96667\n",
            "Epoch [9960/10000], loss: 0.07972 acc: 1.00000 val_loss: 0.15407, val_acc: 0.96667\n",
            "Epoch [9970/10000], loss: 0.07966 acc: 1.00000 val_loss: 0.15403, val_acc: 0.96667\n",
            "Epoch [9980/10000], loss: 0.07961 acc: 1.00000 val_loss: 0.15399, val_acc: 0.96667\n",
            "Epoch [9990/10000], loss: 0.07955 acc: 1.00000 val_loss: 0.15395, val_acc: 0.96667\n"
          ]
        }
      ]
    }
  ]
}